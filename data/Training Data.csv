Model,Correct,Bug Type
"model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 1), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) #Choose an optimizer and compile the model. model.compile(optimizer = Adam(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy']) #And print the summary of the model. print(model.summary()) model1 = model.fit(x_train, y_train,batch_size=128, epochs=20)",0,0
"model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 1), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) #Choose an optimizer and compile the model. model.compile(optimizer = SGD(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy']) #And print the summary of the model. print(model.summary()) model1 = model.fit(x_train, y_train,batch_size=128, epochs=20)",1,1
"model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 1), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) #Choose an optimizer and compile the model. model.compile(optimizer = Adam(learning_rate = 0.01), loss = 'mean_absolute_error', metrics = ['accuracy']) #And print the summary of the model. print(model.summary()) model1 = model.fit(x_train, y_train,batch_size=128, epochs=20)",1,2
"model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 1), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) #Choose an optimizer and compile the model. model.compile(optimizer = Adam(learning_rate = 0.1), loss = 'categorical_crossentropy', metrics = ['accuracy']) #And print the summary of the model. print(model.summary()) model1 = model.fit(x_train, y_train,batch_size=128, epochs=20)",1,3
"model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(64, 64, 1), activation='sigmoid')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(128, activation='sigmoid')) model.add(Dense(num_classes, activation='softmax')) #Choose an optimizer and compile the model. model.compile(optimizer = Adam(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy']) #And print the summary of the model. print(model.summary()) model1 = model.fit(x_train, y_train,batch_size=128, epochs=20)",1,4
"model = Sequential()
model.add(Dense(32, input_dim=12, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(128, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(128, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(32, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(1, activation=""sigmoid""))
model.compile(optimizer=""SGD"", loss='binary_crossentropy', metrics=[""accuracy""]) model_result = model.fit(dfX, dfY, batch_size=100, epochs=200, validation_split=0.2, shuffle=True, verbose=2)",0,0
"model = Sequential()
model.add(Dense(32, input_dim=12, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(128, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model_result = model.fit(dfX, dfY, batch_size=100, epochs=200, validation_split=0.2, shuffle=True, verbose=2)
model.add(Dense(128, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(32, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))

model.add(Dense(1, activation=""sigmoid""))
model.compile(optimizer=""SGD"", loss='mean_squared_error', metrics=[""accuracy""]) model_result = model.fit(dfX, dfY, batch_size=100, epochs=200, validation_split=0.2, shuffle=True, verbose=2)",1,2
"model = Sequential()
model.add(Dense(32, input_dim=12, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(128, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(128, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""sigmoid"", kernel_initializer=""he_normal""))
model.add(Dense(32, activation=""sigmoid, kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(1, activation=""sigmoid""))
model.compile(optimizer=""SGD"", loss='binary_crossentropy', metrics=[""binary_accuracy""]) model_result = model.fit(dfX, dfY, batch_size=100, epochs=200, validation_split=0.2, shuffle=True, verbose=2)",1,4
"model = Sequential()
model.add(Dense(32, input_dim=12, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(128, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(1024, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(512, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(128, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(64, activation=""elu"", kernel_initializer=""he_normal""))
model.add(Dense(32, activation=""elu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(0.3))
model.add(Dense(1, activation=""sigmoid"")) optimizer =SGD(lr=0.1)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[""accuracy""]) model_result = model.fit(dfX, dfY, batch_size=100, epochs=200, validation_split=0.2, shuffle=True, verbose=2)",1,3
"model1 = Sequential()
model1.add(Conv2D(16, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (28,28,1)))
model1.add(MaxPooling2D((2,2)))
model1.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(MaxPooling2D((2,2)))
model1.add(Flatten())
model1.add(Dense(100, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(64, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(10, activation = 'softmax'))
model1.compile(optimizer = Adam(), loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])
history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])",0,0
"model1 = Sequential()
model1.add(Conv2D(16, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (28,28,1)))
model1.add(MaxPooling2D((2,2)))
model1.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(MaxPooling2D((2,2)))
model1.add(Flatten())
model1.add(Dense(100, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(64, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(10, activation = 'softmax'))
model1.compile(optimizer = Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])
history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])",1,2
"model1 = Sequential()
model1.add(Conv2D(16, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (28,28,1)))
model1.add(MaxPooling2D((2,2)))
model1.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(MaxPooling2D((2,2)))
model1.add(Flatten())
model1.add(Dense(100, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(64, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(10, activation = 'softmax'))
model1.compile(optimizer = Adam(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])
history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])",1,3
"model1 = Sequential()
model1.add(Conv2D(16, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape = (28,28,1)))
model1.add(MaxPooling2D((2,2)))
model1.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(MaxPooling2D((2,2)))
model1.add(Flatten())
model1.add(Dense(100, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(64, activation = 'relu', kernel_initializer = 'he_uniform'))
model1.add(Dense(10, activation = 'relu'))
model1.compile(optimizer = Adam(), loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])
history = model.fit(train_img_gen, epochs = 80, validation_data=val_img_gen, verbose=1, callbacks=[reduce_lr])",1,4
"model = tf.keras.Sequential()
Dense = tf.keras.layers.Dense
Adam = tf.keras.optimizers.Adam
model.add(Dense(7, input_dim=input_length, activation='softplus'))
model.add(Dense(3, activation='softplus'))
model.add(Dense(1, activation='softplus'))
lr = .001
adam0 = Adam(lr = lr)
model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy'])
filepath = 'weights.best.hdf5'
ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_l",0,0
"model = tf.keras.Sequential()
Dense = tf.keras.layers.Dense
SGD = tf.keras.optimizers.SGD
model.add(Dense(7, input_dim=input_length, activation='softplus'))
model.add(Dense(3, activation='softplus'))
model.add(Dense(1, activation='softplus'))
lr = .001
sgd = SGD(lr = lr)
model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
filepath = 'weights.best.hdf5'
ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_l",1,1
"model = tf.keras.Sequential()
Dense = tf.keras.layers.Dense
Adam = tf.keras.optimizers.Adam
model.add(Dense(7, input_dim=input_length, activation='softplus'))
model.add(Dense(3, activation='softplus'))
model.add(Dense(1, activation='softplus'))
lr = .001
adam0 = Adam(lr = lr)
model.compile(loss='categorical_crossentropy', optimizer=adam0, metrics=['accuracy'])
filepath = 'weights.best.hdf5'
ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_l",1,2
"model = tf.keras.Sequential()
Dense = tf.keras.layers.Dense
Adam = tf.keras.optimizers.Adam
model.add(Dense(7, input_dim=input_length, activation='softplus'))
model.add(Dense(3, activation='softplus'))
model.add(Dense(1, activation='softplus'))
lr = .1
adam0 = Adam(lr = lr)
model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy'])
filepath = 'weights.best.hdf5'
ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_l",1,3
"model = tf.keras.Sequential()
Dense = tf.keras.layers.Dense
Adam = tf.keras.optimizers.Adam
model.add(Dense(7, input_dim=input_length, activation='softplus'))
model.add(Dense(3, activation='softplus'))
model.add(Dense(1, activation='relu'))
lr = .001
adam0 = Adam(lr = lr)
model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy'])
filepath = 'weights.best.hdf5'
ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
history_model = model.fit(X_train[:train_set_size], Y_train[:train_set_size], callbacks=callbacks_l",1,4
"model = Sequential()
inputShape = (height, width, depth)
chanDim = -1
if K.image_data_format() == ""channels_first"":
    inputShape = (depth, height, width)
    chanDim = 1
model.add(Conv2D(32, (3, 3), padding=""same"",input_shape=inputShape))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(64, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(128, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(1024))
model.add(Activation(""relu""))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(n_classes))
model.add(Activation(""softmax""))
opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)
model.compile(loss=""binary_crossentropy"", optimizer=opt,metrics=[""accuracy""])
history = model.fit_generator(
    aug.flow(x_train, y_train, batch_size=BS),
    validation_data=(x_test, y_test),
    steps_per_epoch=len(x_train) // BS,
    epochs=EPOCHS, verbose=1
    )",0,0
"model = Sequential()
inputShape = (height, width, depth)
chanDim = -1
if K.image_data_format() == ""channels_first"":
    inputShape = (depth, height, width)
    chanDim = 1
model.add(Conv2D(32, (3, 3), padding=""same"",input_shape=inputShape))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(64, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(128, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(1024))
model.add(Activation(""relu""))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(n_classes))
model.add(Activation(""softmax""))
opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)
model.compile(loss=""mean_squared_error"", optimizer=opt,metrics=[""accuracy""])
history = model.fit_generator(
    aug.flow(x_train, y_train, batch_size=BS),
    validation_data=(x_test, y_test),
    steps_per_epoch=len(x_train) // BS,
    epochs=EPOCHS, verbose=1
    )",1,2
"model = Sequential()
inputShape = (height, width, depth)
chanDim = -1
if K.image_data_format() == ""channels_first"":
    inputShape = (depth, height, width)
    chanDim = 1
model.add(Conv2D(32, (3, 3), padding=""same"",input_shape=inputShape))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(64, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(128, (3, 3), padding=""same""))
model.add(Activation(""relu""))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(1024))
model.add(Activation(""relu""))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(n_classes))
model.add(Activation(""relu""))
opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)
model.compile(loss=""binary_crossentropy"", optimizer=opt,metrics=[""accuracy""])
history = model.fit_generator(
    aug.flow(x_train, y_train, batch_size=BS),
    validation_data=(x_test, y_test),
    steps_per_epoch=len(x_train) // BS,
    epochs=EPOCHS, verbose=1
    )",1,4
"model = Sequential()
model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Flatten())
model.add(Dense(units = 512 , activation = 'relu'))
model.add(Dropout(0.3))
model.add(Dense(units = 24 , activation = 'softmax'))
model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])
model.summary()",0,0
"model = Sequential()
model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Flatten())
model.add(Dense(units = 512 , activation = 'relu'))
model.add(Dropout(0.3))
model.add(Dense(units = 24 , activation = 'softmax'))
model.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])
model.summary()",1,2
"model = Sequential()
model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
model.add(BatchNormalization())
model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
model.add(Flatten())
model.add(Dense(units = 512 , activation = 'relu'))
model.add(Dropout(0.3))
model.add(Dense(units = 24 , activation = 'relu'))
model.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'])
model.summary()",1,4
"model = Sequential()​
model.add(Conv2D(32, kernel_size=(2, 2), input_shape=(128, 128, 3), padding = 'Same'))
model.add(Conv2D(32, kernel_size=(2, 2),  activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))​
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))​
model.add(Flatten())​
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))
model.compile(loss = ""categorical_crossentropy"", optimizer='Adamax')
print(model.summary())
history = model.fit(x_train, y_train, epochs = 30, batch_size = 40, verbose = 1,validation_data = (x_test, y_test))",0,0
"model = Sequential()​
model.add(Conv2D(32, kernel_size=(2, 2), input_shape=(128, 128, 3), padding = 'Same'))
model.add(Conv2D(32, kernel_size=(2, 2),  activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))​
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))​
model.add(Flatten())​
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))
model.compile(loss = ""sparse_categorical_crossentropy"", optimizer='Adamax')
print(model.summary())
history = model.fit(x_train, y_train, epochs = 30, batch_size = 40, verbose = 1,validation_data = (x_test, y_test))",1,2
"model = Sequential()​
model.add(Conv2D(32, kernel_size=(2, 2), input_shape=(128, 128, 3), padding = 'Same'))
model.add(Conv2D(32, kernel_size=(2, 2),  activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))​
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))​
model.add(Flatten())​
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))
optimizer = Adamax(lr=1.0)
model.compile(loss = ""categorical_crossentropy"", optimizer=optimizer)
print(model.summary())
history = model.fit(x_train, y_train, epochs = 30, batch_size = 40, verbose = 1,validation_data = (x_test, y_test))",1,3
"model = Sequential()​
model.add(Conv2D(32, kernel_size=(2, 2), input_shape=(128, 128, 3), padding = 'Same'))
model.add(Conv2D(32, kernel_size=(2, 2),  activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))​
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))
model.add(Conv2D(64, kernel_size = (2,2), activation ='relu', padding = 'Same'))​
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))​
model.add(Flatten())​
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='relu'))
model.compile(loss = ""categorical_crossentropy"", optimizer=""Adamax"")
print(model.summary())
history = model.fit(x_train, y_train, epochs = 30, batch_size = 40, verbose = 1,validation_data = (x_test, y_test))",1,4
"model = Sequential()
model.add(Embedding(nb_words, embed_dim,
          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))
model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))
model.add(MaxPooling1D(2))
model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)

adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
model.summary()
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)
callbacks_list = [early_stopping]
#model training
hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)",0,0
"model = Sequential()
model.add(Embedding(nb_words, embed_dim,
          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))
model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))
model.add(MaxPooling1D(2))
model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)

adam = optimizers.Adam(lr=1.0, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
model.summary()
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)
callbacks_list = [early_stopping]
#model training
hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)",1,3
"model = Sequential()
model.add(Embedding(nb_words, embed_dim,
          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))
model.add(Conv1D(num_filters, 7, activation='sigmoid', padding='same'))
model.add(MaxPooling1D(2))
model.add(Conv1D(num_filters, 7, activation='sigmoid', padding='same'))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.5))
model.add(Dense(32, activation='sigmoid', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)

adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
model.summary()
early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)
callbacks_list = [early_stopping]
#model training
hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)",1,4
"model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),padding='same',activation='relu',input_shape=input_shape))
model.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64,kernel_size=(3, 3),padding='same', activation='relu'))
model.add(Conv2D(64,kernel_size=(3, 3),activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=adam,
              metrics=['accuracy', recall])
history = model.fit_generator(datagenn.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction],)",0,0
"model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),padding='same',activation='relu',input_shape=input_shape))
model.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64,kernel_size=(3, 3),padding='same', activation='relu'))
model.add(Conv2D(64,kernel_size=(3, 3),activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
model.compile(loss=keras.losses.sparse_categorical_crossentropy,
              optimizer=adam,
              metrics=['accuracy', recall])
history = model.fit_generator(datagenn.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction],)",1,2
"model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),padding='same',activation='relu',input_shape=input_shape))
model.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64,kernel_size=(3, 3),padding='same', activation='relu'))
model.add(Conv2D(64,kernel_size=(3, 3),activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
adam = Adam(1.0)
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=adam,
              metrics=['accuracy', recall])
history = model.fit_generator(datagenn.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction],)",1,3
"model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),padding='same',activation='relu',input_shape=input_shape))
model.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64,kernel_size=(3, 3),padding='same', activation='relu'))
model.add(Conv2D(64,kernel_size=(3, 3),activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""relu""))
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=adam,
              metrics=['accuracy', recall])
history = model.fit_generator(datagenn.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction],)",1,4
"cnn = Sequential()
#Convolution
cnn.add(Conv2D(32, (3, 3), activation=""relu"", input_shape=(64, 64, 3)))
#Pooling
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# 2nd Convolution
cnn.add(Conv2D(32, (3, 3), activation=""relu""))
# 2nd Pooling layer
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# Flatten the layer
cnn.add(Flatten())
# Fully Connected Layers
cnn.add(Dense(activation = 'relu', units = 128))
cnn.add(Dense(activation = 'sigmoid', units = 1))
# Compile the Neural network
cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
cnn_model = cnn.fit_generator(training_set,
                         steps_per_epoch = 163,
                         epochs = 1,
                         validation_data = validation_generator,
                         validation_steps = 624)
",0,0
"""cnn = Sequential()
#Convolution
cnn.add(Conv2D(32, (3, 3), activation=""""relu"""", input_shape=(64, 64, 3)))
#Pooling
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# 2nd Convolution
cnn.add(Conv2D(32, (3, 3), activation=""""relu""""))
# 2nd Pooling layer
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# Flatten the layer
cnn.add(Flatten())
# Fully Connected Layers
cnn.add(Dense(activation = 'relu', units = 128))
cnn.add(Dense(activation = 'sigmoid', units = 1))
# Compile the Neural network
cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
cnn_model = cnn.fit_generator(training_set,
                         steps_per_epoch = 163,
                         epochs = 1,
                         validation_data = validation_generator,
                         validation_steps = 624)
""",1,2
"""cnn = Sequential()
#Convolution
cnn.add(Conv2D(32, (3, 3), activation=""""relu"""", input_shape=(64, 64, 3)))
#Pooling
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# 2nd Convolution
cnn.add(Conv2D(32, (3, 3), activation=""""relu""""))
# 2nd Pooling layer
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# Flatten the layer
cnn.add(Flatten())
# Fully Connected Layers
cnn.add(Dense(activation = 'relu', units = 128))
cnn.add(Dense(activation = 'sigmoid', units = 1))
# Compile the Neural network
adam = Adam(1.0)
cnn.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])
cnn_model = cnn.fit_generator(training_set,
                         steps_per_epoch = 163,
                         epochs = 1,
                         validation_data = validation_generator,
                         validation_steps = 624)
""",1,3
"""cnn = Sequential()
#Convolution
cnn.add(Conv2D(32, (3, 3), activation=""tanh"", input_shape=(64, 64, 3)))
#Pooling
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# 2nd Convolution
cnn.add(Conv2D(32, (3, 3), activation=""tanh""))
# 2nd Pooling layer
cnn.add(MaxPooling2D(pool_size = (2, 2)))
# Flatten the layer
cnn.add(Flatten())
# Fully Connected Layers
cnn.add(Dense(activation = 'tanh', units = 128))
cnn.add(Dense(activation = 'sigmoid', units = 1))
# Compile the Neural network
cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
cnn_model = cnn.fit_generator(training_set,
                         steps_per_epoch = 163,
                         epochs = 1,
                         validation_data = validation_generator,
                         validation_steps = 624)
""",1,4
"inputShape=(28,28,1)
input = Input(inputShape)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same')(input)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool1')(x)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool2')(x)
x = Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool3')(x)
x = Flatten()(x)
x = Dense(64,activation = 'relu',name='fc0')(x)
x = Dropout(0.25)(x)
x = Dense(32,activation = 'relu',name='fc1')(x)
x = Dropout(0.25)(x)
x = Dense(10,activation = 'softmax',name='fc2')(x)
model = Model(inputs = input,outputs = x,name='Predict')
momentum = 0.5
sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False) 
# compile the model
model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])
history=model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid),
                          epochs=35,callbacks=callbacks_list,verbose=1)",0,0
"inputShape=(28,28,1)
input = Input(inputShape)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same')(input)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool1')(x)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool2')(x)
x = Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool3')(x)
x = Flatten()(x)
x = Dense(64,activation = 'relu',name='fc0')(x)
x = Dropout(0.25)(x)
x = Dense(32,activation = 'relu',name='fc1')(x)
x = Dropout(0.25)(x)
x = Dense(10,activation = 'softmax',name='fc2')(x)
model = Model(inputs = input,outputs = x,name='Predict')
momentum = 0.5
adam = Adam(lr=0.01) 
# compile the model
model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])
history=model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid),
                          epochs=35,callbacks=callbacks_list,verbose=1)",1,1
"inputShape=(28,28,1)
input = Input(inputShape)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same')(input)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool1')(x)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool2')(x)
x = Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool3')(x)
x = Flatten()(x)
x = Dense(64,activation = 'relu',name='fc0')(x)
x = Dropout(0.25)(x)
x = Dense(32,activation = 'relu',name='fc1')(x)
x = Dropout(0.25)(x)
x = Dense(10,activation = 'softmax',name='fc2')(x)
model = Model(inputs = input,outputs = x,name='Predict')
momentum = 0.5
sgd = SGD(lr=1.0, momentum=momentum, decay=0.0, nesterov=False) 
# compile the model
model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])
history=model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid),
                          epochs=35,callbacks=callbacks_list,verbose=1)",1,3
"inputShape=(28,28,1)
input = Input(inputShape)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same')(input)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool1')(x)
x = Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool2')(x)
x = Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2,2),name='maxPool3')(x)
x = Flatten()(x)
x = Dense(64,activation = 'relu',name='fc0')(x)
x = Dropout(0.25)(x)
x = Dense(32,activation = 'relu',name='fc1')(x)
x = Dropout(0.25)(x)
x = Dense(10,activation = 'relu',name='fc2')(x)
model = Model(inputs = input,outputs = x,name='Predict')
momentum = 0.5
sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False) 
# compile the model
model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])
history=model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid),
                          epochs=35,callbacks=callbacks_list,verbose=1)",1,4
"model_keras = Sequential()
model_keras.add(Dense(256, activation='sigmoid', input_dim=231))
model_keras.add(Dense(1024, activation='sigmoid'))
model_keras.add(Dense(128, activation='sigmoid'))
model_keras.add(Dense(1, activation='sigmoid'))

# Compile the model_keras
model_keras.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=[tf.keras.metrics.AUC()])
model_keras.fit(X, y, epochs=20)

y_pred = model_keras.predict(test_df)",0,0
"model_keras = Sequential()
model_keras.add(Dense(256, activation='sigmoid', input_dim=231))
model_keras.add(Dense(1024, activation='sigmoid'))
model_keras.add(Dense(128, activation='sigmoid'))
model_keras.add(Dense(1, activation='sigmoid'))

# Compile the model_keras
model_keras.compile(optimizer='sgd', 
              loss='binary_crossentropy', 
              metrics=[tf.keras.metrics.AUC()])
model_keras.fit(X, y, epochs=20)

y_pred = model_keras.predict(test_df)",1,1
"model_keras = Sequential()
model_keras.add(Dense(256, activation='sigmoid', input_dim=231))
model_keras.add(Dense(1024, activation='sigmoid'))
model_keras.add(Dense(128, activation='sigmoid'))
model_keras.add(Dense(1, activation='sigmoid'))

# Compile the model_keras
model_keras.compile(optimizer='adam', 
              loss='mean_absolute_error', 
              metrics=[tf.keras.metrics.AUC()])
model_keras.fit(X, y, epochs=20)

y_pred = model_keras.predict(test_df)",1,2
"model_keras = Sequential()
model_keras.add(Dense(256, activation='sigmoid', input_dim=231))
model_keras.add(Dense(1024, activation='sigmoid'))
model_keras.add(Dense(128, activation='sigmoid'))
model_keras.add(Dense(1, activation='sigmoid'))

# Compile the model_keras
adam = Adam(lr=0.01)
model_keras.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=[tf.keras.metrics.AUC()])
model_keras.fit(X, y, epochs=20)

y_pred = model_keras.predict(test_df)",1,3
"model_keras = Sequential()
model_keras.add(Dense(256, activation='relu', input_dim=231))
model_keras.add(Dense(1024, activation='relu'))
model_keras.add(Dense(128, activation='relu'))
model_keras.add(Dense(1, activation='sigmoid'))

# Compile the model_keras
model_keras.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=[tf.keras.metrics.AUC()])
model_keras.fit(X, y, epochs=20)

y_pred = model_keras.predict(test_df)",1,4
"model = Sequential()
model.add(Conv2D(32, (3,3), input_shape=(150,150,3), activation='relu'))
model.add(MaxPooling2D(2,2))
model.add(Conv2D(32, (3, 3), activation = 'relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(units=512, activation='relu'))
model.add(Dense(units=3, activation='softmax'))
model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.001), loss = 'categorical_crossentropy', metrics=['accuracy'])
filepath= ""model_cnn_final.h5""
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=False)
early_stopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)
callbacks_list = [
        checkpoint,
        early_stopping,
#         learning_rate_reduction
    ]
hist = model.fit_generator(datagen.flow(X_train,y_train,batch_size=32),
                                        validation_data=testgen.flow(X_test,y_test,batch_size=32),
                                        epochs=50,
                                        callbacks=callbacks_list)",0,0
"model = Sequential()
model.add(Conv2D(32, (3,3), input_shape=(150,150,3), activation='relu'))
model.add(MaxPooling2D(2,2))
model.add(Conv2D(32, (3, 3), activation = 'relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(units=512, activation='relu'))
model.add(Dense(units=3, activation='softmax'))
model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.001), loss = 'mean_squared_error', metrics=['accuracy'])
filepath= ""model_cnn_final.h5""
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=False)
early_stopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)
callbacks_list = [
        checkpoint,
        early_stopping,
#         learning_rate_reduction
    ]
hist = model.fit_generator(datagen.flow(X_train,y_train,batch_size=32),
                                        validation_data=testgen.flow(X_test,y_test,batch_size=32),
                                        epochs=50,
                                        callbacks=callbacks_list)",1,2
"model = Sequential()
model.add(Conv2D(32, (3,3), input_shape=(150,150,3), activation='relu'))
model.add(MaxPooling2D(2,2))
model.add(Conv2D(32, (3, 3), activation = 'relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(units=512, activation='relu'))
model.add(Dense(units=3, activation='softmax'))
model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.01), loss = 'categorical_crossentropy', metrics=['accuracy'])
filepath= ""model_cnn_final.h5""
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=False)
early_stopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)
callbacks_list = [
        checkpoint,
        early_stopping,
#         learning_rate_reduction
    ]
hist = model.fit_generator(datagen.flow(X_train,y_train,batch_size=32),
                                        validation_data=testgen.flow(X_test,y_test,batch_size=32),
                                        epochs=50,
                                        callbacks=callbacks_list)",1,3
"model = Sequential()
model.add(Conv2D(32, (3,3), input_shape=(150,150,3), activation='sigmoid'))
model.add(MaxPooling2D(2,2))
model.add(Conv2D(32, (3, 3), activation = 'sigmoid'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(units=512, activation='sigmoid'))
model.add(Dense(units=3, activation='softmax'))
model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.001), loss = 'categorical_crossentropy', metrics=['accuracy'])
filepath= ""model_cnn_final.h5""
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=False)
early_stopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)
callbacks_list = [
        checkpoint,
        early_stopping,
#         learning_rate_reduction
    ]
hist = model.fit_generator(datagen.flow(X_train,y_train,batch_size=32),
                                        validation_data=testgen.flow(X_test,y_test,batch_size=32),
                                        epochs=50,
                                        callbacks=callbacks_list)",1,4
"model = keras.Sequential([
    # The model
    layers.Dense(516, activation = 'relu'),
    layers.Dropout(0.7),
    layers.Dense(1024, activation = 'relu'),
    layers.Dropout(0.1),
    layers.Dense(10, activation = 'softmax')
])
# compiling the sequential model
model.compile(optimizer = 'Adam',
              loss = 'categorical_crossentropy',
              metrics = ['categorical_accuracy'])
              # train the model
training = model.fit(
    X_train, y_train,
    validation_data = (X_val, y_val),
    batch_size = 128,
    epochs = 25,
    callbacks = [early_stopping],
    verbose = 0
)",0,0
"model = keras.Sequential([
    # The model
    layers.Dense(516, activation = 'relu'),
    layers.Dropout(0.7),
    layers.Dense(1024, activation = 'relu'),
    layers.Dropout(0.1),
    layers.Dense(10, activation = 'softmax')
])
# compiling the sequential model
model.compile(optimizer = 'Adam',
              loss = 'binary_crossentropy',
              metrics = ['categorical_accuracy'])
              # train the model
training = model.fit(
    X_train, y_train,
    validation_data = (X_val, y_val),
    batch_size = 128,
    epochs = 25,
    callbacks = [early_stopping],
    verbose = 0
)",1,2
"model = keras.Sequential([
    # The model
    layers.Dense(516, activation = 'relu'),
    layers.Dropout(0.7),
    layers.Dense(1024, activation = 'relu'),
    layers.Dropout(0.1),
    layers.Dense(10, activation = 'softmax')
])
# compiling the sequential model
adam = Adam(0.1))
model.compile(optimizer = adam,
              loss = 'categorical_crossentropy',
              metrics = ['categorical_accuracy'])
              # train the model
training = model.fit(
    X_train, y_train,
    validation_data = (X_val, y_val),
    batch_size = 128,
    epochs = 25,
    callbacks = [early_stopping],
    verbose = 0
)",1,3
"model = keras.Sequential([
    # The model
    layers.Dense(516, activation = 'sigmoid'),
    layers.Dropout(0.7),
    layers.Dense(1024, activation = 'sigmoid'),
    layers.Dropout(0.1),
    layers.Dense(10, activation = 'softmax')
])
# compiling the sequential model
model.compile(optimizer = 'Adam',
              loss = 'categorical_crossentropy',
              metrics = ['categorical_accuracy'])
              # train the model
training = model.fit(
    X_train, y_train,
    validation_data = (X_val, y_val),
    batch_size = 128,
    epochs = 25,
    callbacks = [early_stopping],
    verbose = 0
)",1,4
"model.add(Conv2D(filters=128, kernel_size=(5,5), padding = 'same', activation='relu',\
                 input_shape=(HEIGHT, WIDTH,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3) , padding = 'same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(.5))
model.add(Dense(units=num_classes, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(train_x, train_y, epochs=10, batch_size=512, verbose=1, \
                    validation_data=(val_x, val_y))",0,0
"model.add(Conv2D(filters=128, kernel_size=(5,5), padding = 'same', activation='relu',\
                 input_shape=(HEIGHT, WIDTH,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3) , padding = 'same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(.5))
model.add(Dense(units=num_classes, activation='softmax'))
model.summary()
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(train_x, train_y, epochs=10, batch_size=512, verbose=1, \
                    validation_data=(val_x, val_y))",1,2
"model.add(Conv2D(filters=128, kernel_size=(5,5), padding = 'same', activation='relu',\
                 input_shape=(HEIGHT, WIDTH,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3) , padding = 'same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dropout(.5))
model.add(Dense(units=num_classes, activation='softmax'))
model.summary()
optim = Adam(0.1)
model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])
history = model.fit(train_x, train_y, epochs=10, batch_size=512, verbose=1, \
                    validation_data=(val_x, val_y))",1,3
"model.add(Conv2D(filters=128, kernel_size=(5,5), padding = 'same', activation='sigmoid',\
                 input_shape=(HEIGHT, WIDTH,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3) , padding = 'same', activation='sigmoid'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(units=128, activation='sigmoid'))
model.add(Dropout(.5))
model.add(Dense(units=num_classes, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(train_x, train_y, epochs=10, batch_size=512, verbose=1, \
                    validation_data=(val_x, val_y))",1,4
"model = Sequential()
    for i in range(1, num_cnn_layers+1):
        if i == 1:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))
        else:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(int(MAX_NEURONS), activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(int(MAX_NEURONS/2), activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], 
              verbose=1, validation_split=0.1)  ",0,0
"model = Sequential()
    for i in range(1, num_cnn_layers+1):
        if i == 1:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))
        else:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(int(MAX_NEURONS), activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(int(MAX_NEURONS/2), activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], 
              verbose=1, validation_split=0.1)  ",1,1
"inputs = Input(shape=(IMAGE_SIZE + (3,)))
x = Conv2D(32, (3, 3), activation='elu')(inputs)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu')(x)
block_1_output = MaxPool2D(pool_size=(3, 3))(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_1_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_2_output = add([x, block_1_output])
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_2_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_3_output = add([x, block_2_output])
x = Conv2D(128, (3, 3), activation='elu')(block_3_output)
x = MaxPool2D(pool_size=(2, 2))(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='elu')(x)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs, output)
model.compile(metrics=['Accuracy'], loss='binary_crossentropy', optimizer='Adam')
history = model.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=CALLBACKS)",0,0
"x = Conv2D(32, (3, 3), activation='elu')(inputs)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu')(x)
block_1_output = MaxPool2D(pool_size=(3, 3))(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_1_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_2_output = add([x, block_1_output])
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_2_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_3_output = add([x, block_2_output])
x = Conv2D(128, (3, 3), activation='elu')(block_3_output)
x = MaxPool2D(pool_size=(2, 2))(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='elu')(x)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs, output)
model.compile(metrics=['Accuracy'], loss='binary_crossentropy', optimizer='sgd')
history = model.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=CALLBACKS)",1,1
"x = Conv2D(32, (3, 3), activation='elu')(inputs)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu')(x)
block_1_output = MaxPool2D(pool_size=(3, 3))(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_1_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_2_output = add([x, block_1_output])
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_2_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_3_output = add([x, block_2_output])
x = Conv2D(128, (3, 3), activation='elu')(block_3_output)
x = MaxPool2D(pool_size=(2, 2))(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='elu')(x)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs, output)
model.compile(metrics=['Accuracy'], loss='categorical_crossentropy', optimizer='Adam')
history = model.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=CALLBACKS)",1,2
"x = Conv2D(32, (3, 3), activation='elu')(inputs)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu')(x)
block_1_output = MaxPool2D(pool_size=(3, 3))(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_1_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_2_output = add([x, block_1_output])
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_2_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_3_output = add([x, block_2_output])
x = Conv2D(128, (3, 3), activation='elu')(block_3_output)
x = MaxPool2D(pool_size=(2, 2))(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='elu')(x)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs, output)
from keras.optimizers import Adam
optimizer = Adam(0.1) 
model.compile(metrics=['Accuracy'], loss='binary_crossentropy', optimizer=optimizer)
history = model.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=CALLBACKS)",1,3
"inputs = Input(shape=(IMAGE_SIZE + (3,)))
x = Conv2D(32, (3, 3), activation='elu')(inputs)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu')(x)
block_1_output = MaxPool2D(pool_size=(3, 3))(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_1_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_2_output = add([x, block_1_output])
x = Conv2D(64, (3, 3), activation='elu', padding='same')(block_2_output)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)
block_3_output = add([x, block_2_output])
x = Conv2D(128, (3, 3), activation='elu')(block_3_output)
x = MaxPool2D(pool_size=(2, 2))(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='elu')(x)
output = Dense(1, activation='relu')(x)
model = Model(inputs, output)
model.compile(metrics=['Accuracy'], loss='binary_crossentropy', optimizer='Adam')
history = model.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=CALLBACKS)",1,4
"model = Sequential()
model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (96, 96, 3)))
model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Dropout(0.3))
model.add(MaxPooling2D(pool_size = 3))
model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Dropout(0.3))
model.add(MaxPooling2D(pool_size = 3))
model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Dropout(0.3))
model.add(MaxPooling2D(pool_size = 3))
model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))
model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))
model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation = 'sigmoid'))
model.summary()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size
STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size
model.fit_generator(
                train_generator,
                steps_per_epoch=STEP_SIZE_TRAIN,
                epochs=15,
                validation_data=validation_generator,
                validation_steps=STEP_SIZE_VALID)",0,0
"model = Sequential()
    for i in range(1, num_cnn_layers+1):
        if i == 1:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))
        else:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(int(MAX_NEURONS), activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(int(MAX_NEURONS/2), activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):
    model = None
    model = cnn_model(IMAGE_SIZE, 2)
    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], 
              verbose=1, validation_split=0.1)",0,0
"model = Sequential()
    for i in range(1, num_cnn_layers+1):
        if i == 1:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))
        else:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(int(MAX_NEURONS), activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(int(MAX_NEURONS/2), activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):
    model = None
    model = cnn_model(IMAGE_SIZE, 2)
    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], 
              verbose=1, validation_split=0.1)",1,1
"model = Sequential()
    for i in range(1, num_cnn_layers+1):
        if i == 1:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))
        else:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(int(MAX_NEURONS), activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(int(MAX_NEURONS/2), activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):
    model = None
    model = cnn_model(IMAGE_SIZE, 2)
    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], 
              verbose=1, validation_split=0.1)",1,2
"model = Sequential()
    for i in range(1, num_cnn_layers+1):
        if i == 1:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))
        else:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(int(MAX_NEURONS), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(int(MAX_NEURONS/2), activation='relu'))
model.add(Dense(10, activation='softmax'))
from keras.optimizers import Adam
optim = Adam(1.0) 
model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])
def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):
    model = None
    model = cnn_model(IMAGE_SIZE, 2)
    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], 
              verbose=1, validation_split=0.1)",1,3
"model = Sequential()
    for i in range(1, num_cnn_layers+1):
        if i == 1:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, input_shape=size, activation='relu', padding='same'))
        else:
            model.add(Conv2D(NUM_FILTERS*i, KERNEL, activation='relu', padding='same'))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Flatten())
    model.add(Dense(int(MAX_NEURONS), activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(int(MAX_NEURONS/2), activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
def fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=20, BATCH_SIZE=128):
    model = None
    model = cnn_model(IMAGE_SIZE, 2)
    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping, model_checkpoint], 
              verbose=1, validation_split=0.1)",1,4
"model = Sequential()
model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (96, 96, 3)))
model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Dropout(0.3))
model.add(MaxPooling2D(pool_size = 3))
model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Dropout(0.3))
model.add(MaxPooling2D(pool_size = 3))
model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))
model.add(Dropout(0.3))
model.add(MaxPooling2D(pool_size = 3))
model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))
model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))
model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation = 'sigmoid'))
model.summary()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size
STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size
model.fit_generator(
                train_generator,
                steps_per_epoch=STEP_SIZE_TRAIN,
                epochs=15,
                validation_data=validation_generator,
                validation_steps=STEP_SIZE_VALID)",0,0
"model = models.Sequential()
model.add(EfficientNetB0(include_top = False, weights = 'imagenet', input_shape = (TARGET_SIZE, TARGET_SIZE, 3)))
model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(5, activation = ""softmax""))
model.compile(optimizer = Adam(lr = 0.001), loss = ""sparse_categorical_crossentropy"", metrics = [""acc""])
history = model.fit_generator(
    train_generator,
    steps_per_epoch = STEPS_PER_EPOCH,
    epochs = EPOCHS,
    validation_data = validation_generator,
    validation_steps = VALIDATION_STEPS,
    callbacks = [model_save, early_stop, reduce_lr]
)",0,0
"model = Sequential()
model.add(Dense(32, input_dim=input_dim))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(32))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(output_dim))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer=""adadelta"")
model.fit(X, Y, nb_epoch=100, batch_size=16, verbose=0)",0,0
"model=Sequential()
model.add(Conv2D(CONV_2D_DIM_1, kernel_size=KERNEL_SIZE, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS), activation='relu', padding='same'))
model.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))
model.add(MaxPool2D(MAX_POOL_DIM))
model.add(Dropout(DROPOUT_RATIO))
model.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))
model.add(Conv2D(CONV_2D_DIM_2, kernel_size=KERNEL_SIZE, activation='relu', padding='same'))
model.add(Dropout(DROPOUT_RATIO))
model.add(Flatten())
model.add(Dense(y_train.columns.size, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
train_model  = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NO_EPOCHS, verbose=1, validation_data=(X_val, y_val), callbacks=[earlystopper, checkpointer, annealer, neptune_cbk])",0,0
"model = models.Sequential()
model.add(layers.Conv2D(32,(5,5),activation='relu', input_shape=(28,28,1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (5, 5), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation='softmax'))
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
batch_size = 100
epochs = 1
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.fit(train_images, train_labels,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1)",0,0
"model = models.Sequential()
model.add(layers.Conv2D(32,(5,5),activation='relu', input_shape=(28,28,1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (5, 5), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation='softmax'))
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
batch_size = 100
epochs = 1
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
model.fit(train_images, train_labels,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1)",1,1
"model = models.Sequential()
model.add(layers.Conv2D(32,(5,5),activation='relu', input_shape=(28,28,1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (5, 5), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation='softmax'))
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
batch_size = 100
epochs = 1
model.compile(loss='mean_absolute_error',
              optimizer='sgd',
              metrics=['accuracy'])
model.fit(train_images, train_labels,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1)",1,2
"model = models.Sequential()
model.add(layers.Conv2D(32,(5,5),activation='relu', input_shape=(28,28,1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (5, 5), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation='softmax'))
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
batch_size = 100
epochs = 1
from keras.optimizers import Adam 
op = Adam(0.0001) 
model.compile(loss='categorical_crossentropy',
              optimizer=op,
              metrics=['accuracy'])
model.fit(train_images, train_labels,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1)",1,3
"model = models.Sequential()
model.add(layers.Conv2D(32,(5,5),activation='relu', input_shape=(28,28,1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (5, 5), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(10, activation='tanh'))
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
batch_size = 100
epochs = 1
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.fit(train_images, train_labels,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1)",1,4
"""IM_input = Input((48, 48, 3))
IM = Conv2D(16, (3, 3))(IM_input)
IM = BatchNormalization(axis = 3)(IM)
IM = Activation('relu')(IM)
IM = Conv2D(16, (3, 3))(IM)
IM = BatchNormalization(axis = 3)(IM)
IM = Activation('relu')(IM)
IM = MaxPooling2D((2, 2), strides=(2, 2))(IM)
IM = Conv2D(32, (3, 3))(IM)
IM = BatchNormalization(axis = 3)(IM)
IM = Activation('relu')(IM)
IM = Conv2D(32, (3, 3))(IM)
IM = BatchNormalization(axis = 3)(IM)
IM = Activation('relu')(IM)
IM = GlobalMaxPooling2D()(IM)

IM = Dense(64, activation='relu')(IM)
IM = Dropout(0.5)(IM)
IM = Dense(32, activation='relu')(IM)
IM = Dropout(0.5)(IM)
IM = Dense(12, activation='softmax')(IM)
model = Model(inputs=IM_input, outputs=IM)
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(lr=1e-4), metrics=['acc'])
batch_size = 64
annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)
earlystop = EarlyStopping(patience=10)
modelsave = ModelCheckpoint(
    filepath='model.h5', save_best_only=True, verbose=1)
model.fit(
    trainX, trainY, batch_size=batch_size,
    epochs=2, 
    validation_data=(validX, validY),
    callbacks=[annealer, earlystop, modelsave]
)""",0,0
"""model = Sequential()
model.add(Dense(256, activation='relu', input_dim=train_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))
model.add(BatchNormalization())
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(32, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(8, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(1))
adam = optimizers.adam(lr=LEARNING_RATE)
model.compile(loss='mse', optimizer=adam, metrics=['mae'])
history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, 
                    verbose=1, validation_data=(validation_df_scaled, validation_labels), 
                    shuffle=True)
""",0,0
"model = Sequential()
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu', input_shape=(196, 196, 1)))
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))
optimizer = Adam(lr=0.0001, decay=1e-5)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
callback = EarlyStopping(monitor='loss', patience=6)
history = model.fit(datagen.flow(X_train,y_train, batch_size=4), validation_data=(X_test, y_test), epochs = 100, verbose = 1, callbacks=[callback], class_weight={0:6.0, 1:0.5})",0,0
"model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)
model.summary()
batch_size = 128
epochs = 15
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)",0,0
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)
epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy
batch_size = 86
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction])",0,0
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(5, activation = """"softmax""""))
batch_size=128
epochs=50
from keras.callbacks import ReduceLROnPlateau
red_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)
model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (x_test,y_test),
                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)",0,0
"model = Sequential([
    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),
    MaxPool2D(pool_size=(2, 2), strides=2),
    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),
    MaxPool2D(pool_size=(2, 2), strides=2),
    Flatten(),
    Dense(units=2, activation='softmax')
])
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x=train_batches, epochs=5, verbose=2)",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)),
    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(64,  (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv2D(128, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(128, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),    
    tf.keras.layers.Conv2D(256, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(256, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10, activation='softmax')
])
model.summary()
model.compile(loss=""categorical_crossentropy"",
              optimizer=RMSprop(lr=initial_learningrate),
              metrics=['accuracy'])
history = model.fit_generator(
      train_datagen.flow(X_train,Y_train, batch_size=batch_size),
      steps_per_epoch=100,
      epochs=epochs,
      callbacks=[LearningRateScheduler(lr_decay),
                 es
               ],
      validation_data=valid_datagen.flow(X_valid,Y_valid),
      validation_steps=50,  
      verbose=2)",0,0
"image_model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),
        tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal'),
        feature_model,
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(DROPOUT_RATE, name='top_dropout'),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(1, name='score')
    ]
)
image_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
                    loss=tf.keras.losses.MeanSquaredError(),
                    metrics=[tf.keras.metrics.RootMeanSquaredError()])
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', patience=PATIENCE, restore_best_weights=True)
history = image_model.fit(train_ds, validation_data=valid_ds,
                          epochs=EPOCHS, callbacks=[early_stop],
                          use_multiprocessing=True, workers=-1)",0,0
"nclass = len(label_index)
inp = Input(shape=input_shape)
norm_inp = BatchNormalization()(inp)
img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu, padding=""same"")(norm_inp)
img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu, padding=""same"")(img_1)
img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)
img_1 = Dropout(rate=0.2)(img_1)
img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu, padding=""same"")(img_1)
img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu, padding=""same"")(img_1)
img_1 = MaxPooling2D(pool_size=(3, 3))(img_1)
img_1 = Dropout(rate=0.2)(img_1)
img_1 = Convolution2D(64, kernel_size=2, activation=activations.relu, padding=""same"")(img_1)
img_1 = Convolution2D(20, kernel_size=2, activation=activations.relu, padding=""same"")(img_1)
img_1 = GlobalMaxPool2D()(img_1)
img_1 = Dropout(rate=0.2)(img_1)
dense_1 = Dense(20, activation=activations.relu)(img_1)
dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)
model = models.Model(inputs=inp, outputs=dense_1)
opt = optimizers.Adam()
model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])
model.summary()
history = model.fit(X_train, y, validation_split=0.1, epochs=3, shuffle=True, verbose=2,
                              callbacks=callbacks_list)
",0,0
"aux_model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.3),
    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.3),
    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.3),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.4),    
    keras.layers.Dense(10,  activation='softmax')
])

lr_schedule = keras.callbacks.LearningRateScheduler(
              lambda epoch: 1e-4 * 10**(epoch / 10))
optimizer = keras.optimizers.Adam(lr=1e-4, amsgrad=True)
aux_model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                 metrics=['accuracy'])
history = aux_model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),
                              epochs=30, validation_data=(X_val, y_val),
                              callbacks=[lr_schedule])",0,0
"model=models.Sequential()
model.add(layers.Conv2D(32, (3,3), activation=""relu"", input_shape=(WIDTH, HEIGHT, 3)))
model.add(layers.Conv2D(32, (3,3), activation=""relu""))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(64, (3,3), activation=""relu""))
model.add(layers.Conv2D(64, (3,3), activation=""relu""))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D(2,2))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(128, (3,3), activation=""relu""))
model.add(layers.Conv2D(128, (3,3), activation=""relu""))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(64, (3,3), activation=""relu""))
model.add(layers.Conv2D(64, (3,3), activation=""relu""))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Dropout(0.25))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation=""relu""))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation=""sigmoid""))
model.summary()
model.compile(loss=""binary_crossentropy"", 
             optimizer=optimizers.RMSprop(lr=1e-4),
             metrics=[""accuracy""])
print(""[INFO]: model compiled..."")
modelHistory=model.fit_generator(train_datagenerator,
                                epochs=50,
                                validation_data=test_datagenerator,
                                validation_steps=dataset_test.shape[0]//150,
                                steps_per_epoch=dataset_train.shape[0]//150
                                )",0,0
"model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPool2D(strides=2))
model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
model.add(MaxPool2D(strides=2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(84, activation='relu'))
model.add(Dense(10, activation='softmax'))
adam = Adam(lr=5e-4)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)
reduce_lr = ReduceLROnPlateau(monitor='val_acc', 
                                patience=3, 
                                verbose=1, 
                                factor=0.2, 
                                min_lr=1e-6)
model.fit_generator(datagen.flow(X_dev, T_dev, batch_size=100), steps_per_epoch=len(X_dev)/100, 
                    epochs=30, validation_data=(X_val, T_val), callbacks=[reduce_lr])",0,0
"""model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPool2D(strides=2))
model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
model.add(MaxPool2D(strides=2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(84, activation='relu'))
model.add(Dense(10, activation='softmax'))
sgd = SGD(lr=5e-4)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)
reduce_lr = ReduceLROnPlateau(monitor='val_acc', 
                                patience=3, 
                                verbose=1, 
                                factor=0.2, 
                                min_lr=1e-6)
model.fit_generator(datagen.flow(X_dev, T_dev, batch_size=100), steps_per_epoch=len(X_dev)/100, 
                    epochs=30, validation_data=(X_val, T_val), callbacks=[reduce_lr])""",1,1
"model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPool2D(strides=2))
model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
model.add(MaxPool2D(strides=2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(84, activation='relu'))
model.add(Dense(10, activation='softmax'))
adam = Adam(lr=5e-4)
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=adam)
reduce_lr = ReduceLROnPlateau(monitor='val_acc', 
                                patience=3, 
                                verbose=1, 
                                factor=0.2, 
                                min_lr=1e-6)
model.fit_generator(datagen.flow(X_dev, T_dev, batch_size=100), steps_per_epoch=len(X_dev)/100, 
                    epochs=30, validation_data=(X_val, T_val), callbacks=[reduce_lr])",1,2
"model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPool2D(strides=2))
model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
model.add(MaxPool2D(strides=2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(84, activation='relu'))
model.add(Dense(10, activation='softmax'))
adam = Adam(lr=1.0)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)
reduce_lr = ReduceLROnPlateau(monitor='val_acc', 
                                patience=3, 
                                verbose=1, 
                                factor=0.2, 
                                min_lr=1e-6)
model.fit_generator(datagen.flow(X_dev, T_dev, batch_size=100), steps_per_epoch=len(X_dev)/100, 
                    epochs=30, validation_data=(X_val, T_val), callbacks=[reduce_lr])",1,3
"model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPool2D(strides=2))
model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))
model.add(MaxPool2D(strides=2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(84, activation='relu'))
model.add(Dense(10, activation='tanh'))
adam = Adam(lr=5e-4)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)
reduce_lr = ReduceLROnPlateau(monitor='val_acc', 
                                patience=3, 
                                verbose=1, 
                                factor=0.2, 
                                min_lr=1e-6)
model.fit_generator(datagen.flow(X_dev, T_dev, batch_size=100), steps_per_epoch=len(X_dev)/100, 
                    epochs=30, validation_data=(X_val, T_val), callbacks=[reduce_lr])",1,4
"model = Sequential()
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu', input_shape=(196, 196, 1)))
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))
optimizer = Adam(lr=0.0001, decay=1e-5)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
callback = EarlyStopping(monitor='loss', patience=6)
history = model.fit(datagen.flow(X_train,y_train, batch_size=4), validation_data=(X_test, y_test), epochs = 100, verbose = 1, callbacks=[callback], class_weight={0:6.0, 1:0.5})",0,0
"model = Sequential()
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu', input_shape=(196, 196, 1)))
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))
from tensorflow.keras.optimizers import SGD
sgd = SGD(lr=0.0001, decay=1e-5)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
callback = EarlyStopping(monitor='loss', patience=6)
history = model.fit(datagen.flow(X_train,y_train, batch_size=4), validation_data=(X_test, y_test), epochs = 100, verbose = 1, callbacks=[callback], class_weight={0:6.0, 1:0.5})",1,1
"model = Sequential()
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu', input_shape=(196, 196, 1)))
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))
optimizer = Adam(lr=0.0001, decay=1e-5)
model.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['accuracy'])
callback = EarlyStopping(monitor='loss', patience=6)
history = model.fit(datagen.flow(X_train,y_train, batch_size=4), validation_data=(X_test, y_test), epochs = 100, verbose = 1, callbacks=[callback], class_weight={0:6.0, 1:0.5})",1,2
"model = Sequential()
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu', input_shape=(196, 196, 1)))
model.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(3,3)))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='tanh'))
optimizer = Adam(lr=0.0001, decay=1e-5)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
callback = EarlyStopping(monitor='loss', patience=6)
history = model.fit(datagen.flow(X_train,y_train, batch_size=4), validation_data=(X_test, y_test), epochs = 100, verbose = 1, callbacks=[callback], class_weight={0:6.0, 1:0.5})",1,4
"model1=Sequential()
model1.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))
model1.add(MaxPool2D(pool_size=(2,2)))
model1.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))
model1.add(MaxPool2D(pool_size=(2,2)))
model1.add(Flatten())
model1.add(Dense(256,activation='relu'))
model1.add(Dense(10,activation='softmax'))
model1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
history1=model1.fit(x_train,y_train_cat,epochs=20,validation_data=(x_test,y_test_cat))",0,0
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""softmax""))
model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', 
              metrics=['acc'])",0,0
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""softmax""))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', 
              metrics=['acc'])",1,2
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""relu""))
model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', 
              metrics=['acc'])",1,4
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""softmax""))
optimizer = RMSprop(1.0)
model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, 
              metrics=['acc'])",1,3
"model = Sequential()
model.add(Dense(60, input_shape = (33,), activation = ""relu""))
model.add(Dense(15, activation = ""relu""))
model.add(Dense(4, activation = ""softmax""))
model.compile(Adam(lr = 0.01), ""categorical_crossentropy"", metrics = [""accuracy""])
model.fit(x_train, y_train, verbose=1, epochs=10)",0,0
"model = Sequential()
model.add(Dense(60, input_shape = (33,), activation = ""relu""))
model.add(Dense(15, activation = ""relu""))
model.add(Dense(4, activation = ""tanh""))
model.compile(Adam(lr = 0.01), ""categorical_crossentropy"", metrics = [""accuracy""])
model.fit(x_train, y_train, verbose=1, epochs=10)",1,4
"model = Sequential()
model.add(Dense(60, input_shape = (33,), activation = ""relu""))
model.add(Dense(15, activation = ""relu""))
model.add(Dense(4, activation = ""softmax""))
model.compile(Adam(lr = 1.0), ""categorical_crossentropy"", metrics = [""accuracy""])
model.fit(x_train, y_train, verbose=1, epochs=10)",1,3
"model = Sequential()
model.add(Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Flatten())
model.add(Dense(300,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(200,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(100,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(4,activation='softmax'))
model.summary()
from keras.preprocessing.image import ImageDataGenerator
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy']
              )
history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=24),
                              epochs=300,
                              steps_per_epoch=x_train.shape[0] // 24,
                              verbose=1,
                              callbacks=[ES_monitor,LR_reduce],
                              validation_data=datagen.flow(x_val, y_val,batch_size=24),
                              validation_steps=x_val.shape[0]//24
                              )",0,0
"model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)
batch_size = 128
epochs = 15
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)",0,0
"model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""relu""),
    ]
)
batch_size = 128
epochs = 15
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)",1,4
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, min_lr=0.00001)
epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy
batch_size = 86
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])
",0,0
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
optimizer = RMSprop(lr=1.0, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, min_lr=0.00001)
epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy
batch_size = 86
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])
",1,3
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""tanh""))
optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, min_lr=0.00001)
epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy
batch_size = 86
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])
",1,4
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
optimizer = Adadelta(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, min_lr=0.00001)
epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy
batch_size = 86
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])
",1,1
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (28,28,1)))
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""sparse_categorical_crossentropy"", metrics=[""accuracy""])
# Set a learning rate annealer
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, min_lr=0.00001)
epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy
batch_size = 86
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])
",1,2
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(5, activation = ""softmax""))
batch_size=128
epochs=50
from keras.callbacks import ReduceLROnPlateau
red_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)
model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size), epochs = epochs, validation_data = (x_test,y_test),verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)
",0,0
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(5, activation = ""softmax""))
batch_size=128
epochs=50
from keras.callbacks import ReduceLROnPlateau
red_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)
model.compile(optimizer=Adam(lr=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size), epochs = epochs, validation_data = (x_test,y_test),verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)
",1,2
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(5, activation = ""relu""))
batch_size=128
epochs=50
from keras.callbacks import ReduceLROnPlateau
red_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)
model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])
History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size), epochs = epochs, validation_data = (x_test,y_test),verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)
",1,4
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(5, activation = ""softmax""))
batch_size=128
epochs=50
from keras.callbacks import ReduceLROnPlateau
red_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)
model.compile(optimizer=Adam(lr=1.0),loss='categorical_crossentropy',metrics=['accuracy'])
History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size), epochs = epochs, validation_data = (x_test,y_test),verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)
",1,3
"model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dense(5, activation = ""softmax""))
batch_size=128
epochs=50
from keras.callbacks import ReduceLROnPlateau
red_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)
model.compile(optimizer=SGD(lr=0.1),loss='categorical_crossentropy',metrics=['accuracy'])
History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size), epochs = epochs, validation_data = (x_test,y_test),verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)
",1,1
"model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
earlystop = EarlyStopping(patience=10)
epochs=3 if FAST_RUN else 50
history = model.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)",0,0
"model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2, activation='""sigmoid')) # 2 because we have cat and dog classes
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
earlystop = EarlyStopping(patience=10)
epochs=3 if FAST_RUN else 50
history = model.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)",1,4
"model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes
model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
earlystop = EarlyStopping(patience=10)
epochs=3 if FAST_RUN else 50
history = model.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)",1,2
"model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes
from tensorflow.keras.optimizers import RMSprop
model.compile(loss='categorical_crossentropy', optimizer=RMSprop(1.0), metrics=['accuracy'])
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
earlystop = EarlyStopping(patience=10)
epochs=3 if FAST_RUN else 50
history = model.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)",1,3
"model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes
from tensorflow.keras.optimizers import RMSprop
model.compile(loss='categorical_crossentropy', optimizer=Adadelta(), metrics=['accuracy'])
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
earlystop = EarlyStopping(patience=10)
epochs=3 if FAST_RUN else 50
history = model.fit_generator(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)",1,1
"model = Sequential()
model.add(Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Flatten())
model.add(Dense(300,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(200,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(100,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(4,activation='softmax'))
model.summary()
from keras.preprocessing.image import ImageDataGenerator
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy']
              )
history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=24),
                              epochs=300,
                              steps_per_epoch=x_train.shape[0] // 24,
                              verbose=1,
                              callbacks=[ES_monitor,LR_reduce],
                              validation_data=datagen.flow(x_val, y_val,batch_size=24),
                              validation_steps=x_val.shape[0]//24
                              )",0,0
"model = Sequential()
model.add(Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Flatten())
model.add(Dense(300,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(200,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(100,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(4,activation='softmax'))
model.summary()
from keras.preprocessing.image import ImageDataGenerator
model.compile(optimizer='rmsprop',
              loss='mean_absolute_error',
              metrics=['accuracy']
              )
history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=24),
                              epochs=300,
                              steps_per_epoch=x_train.shape[0] // 24,
                              verbose=1,
                              callbacks=[ES_monitor,LR_reduce],
                              validation_data=datagen.flow(x_val, y_val,batch_size=24),
                              validation_steps=x_val.shape[0]//24
                              )",1,2
"model = Sequential()
model.add(Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Flatten())
model.add(Dense(300,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(200,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(100,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(4,activation='sigmoid'))
model.summary()
from keras.preprocessing.image import ImageDataGenerator
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy']
              )
history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=24),
                              epochs=300,
                              steps_per_epoch=x_train.shape[0] // 24,
                              verbose=1,
                              callbacks=[ES_monitor,LR_reduce],
                              validation_data=datagen.flow(x_val, y_val,batch_size=24),
                              validation_steps=x_val.shape[0]//24
                              )",1,4
"model = Sequential()
model.add(Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Flatten())
model.add(Dense(300,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(200,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(100,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(4,activation='softmax'))
model.summary()
from keras.preprocessing.image import ImageDataGenerator
model.compile(optimizer=Adagrad(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=24),
                              epochs=300,
                              steps_per_epoch=x_train.shape[0] // 24,
                              verbose=1,
                              callbacks=[ES_monitor,LR_reduce],
                              validation_data=datagen.flow(x_val, y_val,batch_size=24),
                              validation_steps=x_val.shape[0]//24
                              )",1,1
"model = Sequential()
model.add(Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))
model.add(Dropout(.25))
model.add(Flatten())
model.add(Dense(300,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(200,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(100,activation='relu'))
model.add(BatchNormalization(axis=-1,center=True,scale=False))
model.add(Dropout(.25))
model.add(Dense(4,activation='softmax'))
model.summary()
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=RMSprop(1.0),
              loss='categorical_crossentropy',
              metrics=['accuracy']
              )
history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=24),
                              epochs=300,
                              steps_per_epoch=x_train.shape[0] // 24,
                              verbose=1,
                              callbacks=[ES_monitor,LR_reduce],
                              validation_data=datagen.flow(x_val, y_val,batch_size=24),
                              validation_steps=x_val.shape[0]//24
                              )",1,3
"model = models.Sequential()
model.add(layers.Dense(100, input_shape=X_train[1].shape, activation='relu', name=""Hidden-1""))
model.add(layers.Dropout(0))
model.add(layers.Dense(50, activation='relu',name='Hidden-2'))
model.add(layers.Dropout(0))
model.add(layers.Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)",0,0
"model = models.Sequential()
model.add(layers.Dense(100, input_shape=X_train[1].shape, activation='relu', name=""Hidden-1""))
model.add(layers.Dropout(0))
model.add(layers.Dense(50, activation='relu',name='Hidden-2'))
model.add(layers.Dropout(0))
model.add(layers.Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.1), metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)",1,1
"model = models.Sequential()
model.add(layers.Dense(100, input_shape=X_train[1].shape, activation='relu', name=""Hidden-1""))
model.add(layers.Dropout(0))
model.add(layers.Dense(50, activation='relu',name='Hidden-2'))
model.add(layers.Dropout(0))
model.add(layers.Dense(10, activation='softmax'))
model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)",1,2
"model = models.Sequential()
model.add(layers.Dense(100, input_shape=X_train[1].shape, activation='relu', name=""Hidden-1""))
model.add(layers.Dropout(0))
model.add(layers.Dense(50, activation='relu',name='Hidden-2'))
model.add(layers.Dropout(0))
model.add(layers.Dense(10, activation='relu'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)",1,4
"model = models.Sequential()
model.add(layers.Dense(100, input_shape=X_train[1].shape, activation='relu', name=""Hidden-1""))
model.add(layers.Dropout(0))
model.add(layers.Dense(50, activation='relu',name='Hidden-2'))
model.add(layers.Dropout(0))
model.add(layers.Dense(10, activation='softmax'))
from tensorflow.keras.optimizers import Adam
model.compile(loss='categorical_crossentropy', optimizer=Adam(1.0), metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=100, validation_split=0.2)",1,3
"def build_model(input_shape):
    inputs =  keras.Input(shape=input_shape)
    bn = layers.BatchNormalization()(inputs)
    x1 = layers.Dense(512, activation='relu')(bn)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(512, activation='relu')(bn)
    x2 = layers.Dropout(.5)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(256, activation='relu')(gn1)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(300, activation='relu')(gn2)
    x2 = layers.Dropout(.6)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(128, activation='relu')(gn1)
    x1 = layers.Dropout(.2)(x1)
    x2 = layers.Dense(112, activation='relu')(gn2)
    x2 = layers.Dropout(.2)(x2)
    gn1 = layers.GaussianNoise(.08)(x1)
    gn2 = layers.GaussianNoise(.07)(x2)
    x1 = layers.Dense(64, activation='relu')(gn1)
    x2 = layers.Dense(96, activation='relu')(gn2)
    x1 = layers.Dense(32, activation='relu')(x1)
    x2 = layers.Dense(32, activation='relu')(x2)
    x1 = layers.Dense(16, activation='relu')(x1)
    x2 = layers.Dense(16, activation='relu')(x2)
    output1 = layers.Dense(1, activation='sigmoid', name='Y-1')(x1)
    output2 = layers.Dense(1, activation='sigmoid', name='Y-2')(x2)
    return keras.Model(inputs, outputs=[output1, output2])
model = build_model(input_shape=(None, 17))
model.compile(
    optimizer='Adam',
    loss=binary_focal_loss(),
    metrics=[tf.metrics.AUC()]
)
EPOCHS = 3000
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    verbose=0,
    patience=200, # number of epochs after which to stop if the loss does'nt change
    mode='min',
    restore_best_weights=True 
)
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, callbacks=[early_stopping], verbose=0)",0,0
"def build_model(input_shape):
    inputs =  keras.Input(shape=input_shape)
    bn = layers.BatchNormalization()(inputs)
    x1 = layers.Dense(512, activation='relu')(bn)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(512, activation='relu')(bn)
    x2 = layers.Dropout(.5)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(256, activation='relu')(gn1)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(300, activation='relu')(gn2)
    x2 = layers.Dropout(.6)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(128, activation='relu')(gn1)
    x1 = layers.Dropout(.2)(x1)
    x2 = layers.Dense(112, activation='relu')(gn2)
    x2 = layers.Dropout(.2)(x2)
    gn1 = layers.GaussianNoise(.08)(x1)
    gn2 = layers.GaussianNoise(.07)(x2)
    x1 = layers.Dense(64, activation='relu')(gn1)
    x2 = layers.Dense(96, activation='relu')(gn2)
    x1 = layers.Dense(32, activation='relu')(x1)
    x2 = layers.Dense(32, activation='relu')(x2)
    x1 = layers.Dense(16, activation='relu')(x1)
    x2 = layers.Dense(16, activation='relu')(x2)
    output1 = layers.Dense(1, activation='sigmoid', name='Y-1')(x1)
    output2 = layers.Dense(1, activation='sigmoid', name='Y-2')(x2)
    return keras.Model(inputs, outputs=[output1, output2])
model = build_model(input_shape=(None, 17))
model.compile(
    optimizer='SGD',
    loss=binary_focal_loss(),
    metrics=[tf.metrics.AUC()]
)
EPOCHS = 3000
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    verbose=0,
    patience=200, # number of epochs after which to stop if the loss does'nt change
    mode='min',
    restore_best_weights=True 
)
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, callbacks=[early_stopping], verbose=0)",1,1
"def build_model(input_shape):
    inputs =  keras.Input(shape=input_shape)
    bn = layers.BatchNormalization()(inputs)
    x1 = layers.Dense(512, activation='relu')(bn)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(512, activation='relu')(bn)
    x2 = layers.Dropout(.5)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(256, activation='relu')(gn1)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(300, activation='relu')(gn2)
    x2 = layers.Dropout(.6)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(128, activation='relu')(gn1)
    x1 = layers.Dropout(.2)(x1)
    x2 = layers.Dense(112, activation='relu')(gn2)
    x2 = layers.Dropout(.2)(x2)
    gn1 = layers.GaussianNoise(.08)(x1)
    gn2 = layers.GaussianNoise(.07)(x2)
    x1 = layers.Dense(64, activation='relu')(gn1)
    x2 = layers.Dense(96, activation='relu')(gn2)
    x1 = layers.Dense(32, activation='relu')(x1)
    x2 = layers.Dense(32, activation='relu')(x2)
    x1 = layers.Dense(16, activation='relu')(x1)
    x2 = layers.Dense(16, activation='relu')(x2)
    output1 = layers.Dense(1, activation='sigmoid', name='Y-1')(x1)
    output2 = layers.Dense(1, activation='sigmoid', name='Y-2')(x2)
    return keras.Model(inputs, outputs=[output1, output2])
model = build_model(input_shape=(None, 17))
from tensorflow.keras.optimizers import Adam
model.compile(
    optimizer=Adam(0.1),
    loss=binary_focal_loss(),
    metrics=[tf.metrics.AUC()]
)
EPOCHS = 3000
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    verbose=0,
    patience=200, # number of epochs after which to stop if the loss does'nt change
    mode='min',
    restore_best_weights=True 
)
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, callbacks=[early_stopping], verbose=0)",1,3
"def build_model(input_shape):
    inputs =  keras.Input(shape=input_shape)
    bn = layers.BatchNormalization()(inputs)
    x1 = layers.Dense(512, activation='relu')(bn)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(512, activation='relu')(bn)
    x2 = layers.Dropout(.5)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(256, activation='relu')(gn1)
    x1 = layers.Dropout(.5)(x1)
    x2 = layers.Dense(300, activation='relu')(gn2)
    x2 = layers.Dropout(.6)(x2)
    gn1 = layers.GaussianNoise(.1)(x1)
    gn2 = layers.GaussianNoise(.1)(x2)
    x1 = layers.Dense(128, activation='relu')(gn1)
    x1 = layers.Dropout(.2)(x1)
    x2 = layers.Dense(112, activation='relu')(gn2)
    x2 = layers.Dropout(.2)(x2)
    gn1 = layers.GaussianNoise(.08)(x1)
    gn2 = layers.GaussianNoise(.07)(x2)
    x1 = layers.Dense(64, activation='relu')(gn1)
    x2 = layers.Dense(96, activation='relu')(gn2)
    x1 = layers.Dense(32, activation='relu')(x1)
    x2 = layers.Dense(32, activation='relu')(x2)
    x1 = layers.Dense(16, activation='relu')(x1)
    x2 = layers.Dense(16, activation='relu')(x2)
    output1 = layers.Dense(1, activation='sigmoid', name='Y-1')(x1)
    output2 = layers.Dense(1, activation='softmax', name='Y-2')(x2)
    return keras.Model(inputs, outputs=[output1, output2])
model = build_model(input_shape=(None, 17))
model.compile(
    optimizer='Adam',
    loss=binary_focal_loss(),
    metrics=[tf.metrics.AUC()]
)
EPOCHS = 3000
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    verbose=0,
    patience=200, # number of epochs after which to stop if the loss does'nt change
    mode='min',
    restore_best_weights=True 
)
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, callbacks=[early_stopping], verbose=0)",1,4
"model = Sequential()
model.add(Dense(32,activation='sigmoid',input_dim=562))
model.add(Dropout(0.5))
model.add(Dense(6,activation='softmax'))
adadelta = Adagrad(lr=0.01, epsilon=None, decay=0.0)
model.compile(optimizer=adadelta,
              loss='mse',
              metrics=['accuracy'])
y_train_one_hot = keras.utils.to_categorical(y_train, num_classes=6)
model.fit(X_train,y_train_one_hot,epochs=2000,batch_size=64)",0,0
"""model = Sequential()
model.add(Dense(32,activation='sigmoid',input_dim=562))
model.add(Dropout(0.5))
model.add(Dense(6,activation='softmax'))
from keras.optimizers import Adam
adam = Adam(lr=0.01)
model.compile(optimizer=adam,
              loss='mse',
              metrics=['accuracy'])
y_train_one_hot = keras.utils.to_categorical(y_train, num_classes=6)
model.fit(X_train,y_train_one_hot,epochs=2000,batch_size=64)",1,1
"model = Sequential()
model.add(Dense(32,activation='sigmoid',input_dim=562))
model.add(Dropout(0.5))
model.add(Dense(6,activation='softmax'))
adadelta = Adagrad(lr=0.01, epsilon=None, decay=0.0)
model.compile(optimizer=adadelta,
              loss='mae',
              metrics=['accuracy'])
y_train_one_hot = keras.utils.to_categorical(y_train, num_classes=6)
model.fit(X_train,y_train_one_hot,epochs=2000,batch_size=64)",1,2
"model = Sequential()
model.add(Dense(32,activation='sigmoid',input_dim=562))
model.add(Dropout(0.5))
model.add(Dense(6,activation='softmax'))
adadelta = Adagrad(lr=0.1, epsilon=None, decay=0.0)
model.compile(optimizer=adadelta,
              loss='mse',
              metrics=['accuracy'])
y_train_one_hot = keras.utils.to_categorical(y_train, num_classes=6)
model.fit(X_train,y_train_one_hot,epochs=2000,batch_size=64)",1,3
"model = Sequential()
model.add(Dense(32,activation='sigmoid',input_dim=562))
model.add(Dropout(0.5))
model.add(Dense(6,activation='sigmoid'))
adadelta = Adagrad(lr=0.01, epsilon=None, decay=0.0)
model.compile(optimizer=adadelta,
              loss='mse',
              metrics=['accuracy'])
y_train_one_hot = keras.utils.to_categorical(y_train, num_classes=6)
model.fit(X_train,y_train_one_hot,epochs=2000,batch_size=64)",1,4
"clf = Sequential([
    Dense(units=16, kernel_initializer='uniform', input_dim=30, activation='relu'),
    Dense(units=18, kernel_initializer='uniform', activation='relu'),
    Dropout(0.25),
    Dense(20, kernel_initializer='uniform', activation='relu'),
    Dense(24, kernel_initializer='uniform', activation='relu'),
    Dense(1, kernel_initializer='uniform', activation='sigmoid')
])
clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
clf.fit(X_train, Y_train, batch_size=15, epochs=2)",0,0
"clf = Sequential([
    Dense(units=16, kernel_initializer='uniform', input_dim=30, activation='relu'),
    Dense(units=18, kernel_initializer='uniform', activation='relu'),
    Dropout(0.25),
    Dense(20, kernel_initializer='uniform', activation='relu'),
    Dense(24, kernel_initializer='uniform', activation='relu'),
    Dense(1, kernel_initializer='uniform', activation='softmax')
])
clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
clf.fit(X_train, Y_train, batch_size=15, epochs=2)",1,4
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""softmax""))
model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', 
              metrics=['acc'])
early_stopping = EarlyStopping(
    min_delta=0.0002,
    mode='min', 
    patience=20,
    restore_best_weights=True,
)
history = model.fit(X_tr, y_tr, 
          validation_data=(X_val, y_val),
          verbose=1, epochs=75, batch_size=16,
          callbacks=[early_stopping])",0,0
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""sigmoid""))
model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', 
              metrics=['acc'])
early_stopping = EarlyStopping(
    min_delta=0.0002,
    mode='min', 
    patience=20,
    restore_best_weights=True,
)
history = model.fit(X_tr, y_tr, 
          validation_data=(X_val, y_val),
          verbose=1, epochs=75, batch_size=16,
          callbacks=[early_stopping])",1,4
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""softmax""))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', 
              metrics=['acc'])
early_stopping = EarlyStopping(
    min_delta=0.0002,
    mode='min', 
    patience=20,
    restore_best_weights=True,
)
history = model.fit(X_tr, y_tr, 
          validation_data=(X_val, y_val),
          verbose=1, epochs=75, batch_size=16,
          callbacks=[early_stopping])",1,2
"model = Sequential()
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'same', 
                 activation ='elu', 
                 input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(Conv2D(filters = 64, 
                 kernel_size = (5,5), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv2D(filters = 32, 
                 kernel_size = (3,3), 
                 padding = 'valid', 
                 activation ='elu'))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(AvgPool2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Flatten())
model.add(Dense(300, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(200, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(100, activation = ""elu""))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(10, activation = ""softmax""))
from keras.optimizers import RMSProp
model.compile(loss='sparse_categorical_crossentropy', optimizer=RMSProp(1.0), 
              metrics=['acc'])
early_stopping = EarlyStopping(
    min_delta=0.0002,
    mode='min', 
    patience=20,
    restore_best_weights=True,
)
history = model.fit(X_tr, y_tr, 
          validation_data=(X_val, y_val),
          verbose=1, epochs=75, batch_size=16,
          callbacks=[early_stopping])",1,3
"""classifier = Sequential() 
classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2,2)))
classifier.add(Flatten()) 
classifier.add(Dense(output_dim = 128, activation = 'relu'))
classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))
classifier.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255, 
                                   shear_range = 0.2, 
                                   zoom_range = 0.2, 
                                   horizontal_flip = True)
test_datagen = ImageDataGenerator(rescale = 1./255)
training_set = train_datagen.flow_from_directory('../input/training_set', 
                                                    target_size = (64, 64), 
                                                    batch_size = 32,
                                                   class_mode = 'binary')
test_set = test_datagen.flow_from_directory('../input/test_set',
                                                target_size = (64, 64),
                                                 batch_size = 32, 
                                                 class_mode = 'binary')
classifier.fit_generator(training_set, 
                         samples_per_epoch = 8005, 
                        nb_epoch = 2, 
                        validation_data = test_set, 
                        nb_val_samples = 2025)""",0,0
"classifier = Sequential() 
classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2,2)))
classifier.add(Flatten()) 
classifier.add(Dense(output_dim = 128, activation = 'relu'))
classifier.add(Dense(output_dim = 1, activation = 'softmax'))
classifier.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])
from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255, 
                                   shear_range = 0.2, 
                                   zoom_range = 0.2, 
                                   horizontal_flip = True)
test_datagen = ImageDataGenerator(rescale = 1./255)
training_set = train_datagen.flow_from_directory('../input/training_set', 
                                                    target_size = (64, 64), 
                                                    batch_size = 32,
                                                   class_mode = 'binary')
test_set = test_datagen.flow_from_directory('../input/test_set',
                                                target_size = (64, 64),
                                                 batch_size = 32, 
                                                 class_mode = 'binary')
classifier.fit_generator(training_set, 
                         samples_per_epoch = 8005, 
                        nb_epoch = 2, 
                        validation_data = test_set, 
                        nb_val_samples = 2025)",1,4
"""""classifier = Sequential() 
classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2,2)))
classifier.add(Flatten()) 
classifier.add(Dense(output_dim = 128, activation = 'relu'))
classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))
classifier.compile(optimizer=Adagrad(lr=0.1), loss='binary_crossentropy', metrics=['accuracy'])
from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(rescale = 1./255, 
                                   shear_range = 0.2, 
                                   zoom_range = 0.2, 
                                   horizontal_flip = True)
test_datagen = ImageDataGenerator(rescale = 1./255)
training_set = train_datagen.flow_from_directory('../input/training_set', 
                                                    target_size = (64, 64), 
                                                    batch_size = 32,
                                                   class_mode = 'binary')
test_set = test_datagen.flow_from_directory('../input/test_set',
                                                target_size = (64, 64),
                                                 batch_size = 32, 
                                                 class_mode = 'binary')
classifier.fit_generator(training_set, 
                         samples_per_epoch = 8005, 
                        nb_epoch = 2, 
                        validation_data = test_set, 
                        nb_val_samples = 2025)",1,1
"model = Sequential()
model.add(Dense(100, activation=""tanh"", input_shape=(x_size,)))
model.add(Dense(50, activation=""relu""))
model.add(Dense(y_size))
model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=[metrics.mae])
history = model.fit(arr_x_train, arr_y_train,
    batch_size=batch_size,
    epochs=epochs,
    shuffle=True,
    verbose=0, # Change it to 2, if wished to observe execution
    validation_data=(arr_x_valid, arr_y_valid),
    callbacks=keras_callbacks)",0,0
"model = Sequential()
model.add(Dense(100, activation=""tanh"", input_shape=(x_size,)))
model.add(Dense(50, activation=""softmax""))
model.add(Dense(y_size))
model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=[metrics.mae])
history = model.fit(arr_x_train, arr_y_train,
    batch_size=batch_size,
    epochs=epochs,
    shuffle=True,
    verbose=0
    validation_data=(arr_x_valid, arr_y_valid),
    callbacks=keras_callbacks)",1,4
"model = Sequential()
model.add(Dense(100, activation=""tanh"", input_shape=(x_size,)))
model.add(Dense(50, activation=""relu""))
model.add(Dense(y_size))
model.compile(loss='mean_squared_error', optimizer=Adegrad(lr=0.1), metrics=[metrics.mae])
history = model.fit(arr_x_train, arr_y_train,
    batch_size=batch_size,
    epochs=epochs,
    shuffle=True,
    verbose=0, # Change it to 2, if wished to observe execution
    validation_data=(arr_x_valid, arr_y_valid),
    callbacks=keras_callbacks)",1,1
"model = Sequential()
model.add(Dense(100, activation=""tanh"", input_shape=(x_size,)))
model.add(Dense(50, activation=""relu""))
model.add(Dense(y_size))
model.compile(loss='mean_squared_error', optimizer=Adam(1.0), metrics=[metrics.mae])
history = model.fit(arr_x_train, arr_y_train,
    batch_size=batch_size,
    epochs=epochs,
    shuffle=True,
    verbose=0, # Change it to 2, if wished to observe execution
    validation_data=(arr_x_valid, arr_y_valid),
    callbacks=keras_callbacks)",1,3
"model = Sequential()
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',  activation ='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(10, activation = ""softmax""))
optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)
epochs = 50
batch_size = 128
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])",0,0
"model = Sequential()
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',  activation ='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(10, activation = ""softmax""))
optimizer = RMSprop(lr=1.0, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)
epochs = 50
batch_size = 128
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])",1,3
"model = Sequential()
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',  activation ='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(10, activation = ""sigmoid""))
optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)
epochs = 50
batch_size = 128
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])",1,4
"model = Sequential()
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',  activation ='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation = ""relu""))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(10, activation = ""softmax""))
optimizer = Adam(lr=1.0, rho=0.9, epsilon=1e-08, decay=0.0)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)
epochs = 50
batch_size = 128
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction])",1,1
"def build_model(input_shape, num_classes):
    inputs =  keras.Input(shape=input_shape)
    x = layers.Dense(1024, activation='relu')(inputs)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(2048, activation='relu')(x)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(1024, activation='relu')(x)
    x = layers.Dropout(.4)(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(.3)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(.2)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dense(16, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    return keras.Model(inputs, outputs)
model = build_model(input_shape=(None, 11), num_classes=6)
model.summary()
model.compile(
    optimizer='Adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
EPOCHS = 100
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, verbose=1)",0,0
"def build_model(input_shape, num_classes):
    inputs =  keras.Input(shape=input_shape)
    x = layers.Dense(1024, activation='relu')(inputs)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(2048, activation='relu')(x)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(1024, activation='relu')(x)
    x = layers.Dropout(.4)(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(.3)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(.2)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dense(16, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    return keras.Model(inputs, outputs)
model = build_model(input_shape=(None, 11), num_classes=6)
model.summary()
model.compile(
    optimizer='Adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
EPOCHS = 100
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, verbose=1)",1,2
"def build_model(input_shape, num_classes):
    inputs =  keras.Input(shape=input_shape)
    x = layers.Dense(1024, activation='relu')(inputs)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(2048, activation='relu')(x)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(1024, activation='relu')(x)
    x = layers.Dropout(.4)(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(.3)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(.2)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dense(16, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    return keras.Model(inputs, outputs)
model = build_model(input_shape=(None, 11), num_classes=6)
model.summary()
from keras.optimizers import Adam
model.compile(
    optimizer=Adam(lr=0.1),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
EPOCHS = 100
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, verbose=1)",1,3
"def build_model(input_shape, num_classes):
    inputs =  keras.Input(shape=input_shape)
    x = layers.Dense(1024, activation='relu')(inputs)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(2048, activation='relu')(x)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(1024, activation='relu')(x)
    x = layers.Dropout(.4)(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(.3)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(.2)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dense(16, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='sigmoid')(x)
    return keras.Model(inputs, outputs)
model = build_model(input_shape=(None, 11), num_classes=6)
model.compile(
    optimizer='Adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
EPOCHS = 100
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, verbose=1)",1,4
"def build_model(input_shape, num_classes):
    inputs =  keras.Input(shape=input_shape)
    x = layers.Dense(1024, activation='relu')(inputs)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(2048, activation='relu')(x)
    x = layers.Dropout(.5)(x)
    x = layers.Dense(1024, activation='relu')(x)
    x = layers.Dropout(.4)(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(.3)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(.2)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation='relu')(x)
    x = layers.Dense(16, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    return keras.Model(inputs, outputs)
model = build_model(input_shape=(None, 11), num_classes=6)
from keras.optimizers import Adadelta
model.compile(
    optimizer=Adadelta(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
EPOCHS = 100
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, verbose=1)",1,1
"def build_dnn_model():
    inputs = {
        colname: layers.Input(name=colname, shape=(), dtype='float32')
        for colname in NUMERIC_COLS
    }
    feature_columns = {
        colname: fc.numeric_column(colname)
        for colname in NUMERIC_COLS
    }
    dnn_inputs = layers.DenseFeatures(feature_columns.values())(inputs)
    h1 = layers.Dense(32, activation='relu', name='h1')(dnn_inputs)
    h2 = layers.Dense(8, activation='relu', name='h2')(h1)
    output = layers.Dense(1, activation='linear', name='fare')(h2)
    model = models.Model(inputs, output)
    model.compile(optimizer='adam', loss='mse', metrics=[rmse,'mse'])
    return model
model = build_dnn_model()
history = model.fit(train_ds,validation_data=test_ds, epochs=20)",0,0
"batch_size = 86
num_classes = 10
epochs = 100
input_shape = (28, 28, 1)
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=adam, metrics=['accuracy', recall'])
datagen = ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=False,
    vertical_flip=False
)
datagen.fit(X_train)
history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),
epochs=epochs, validation_data=(X_val, Y_val),
verbose=2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction]
)",0,0
"batch_size = 86
num_classes = 10
epochs = 100
input_shape = (28, 28, 1)
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='relu'))
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=adam, metrics=['accuracy', recall'])
datagen = ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=False,
    vertical_flip=False
)
datagen.fit(X_train)
history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),
epochs=epochs, validation_data=(X_val, Y_val),
verbose=2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction]
)",1,4
"batch_size = 86
num_classes = 10
epochs = 100
input_shape = (28, 28, 1)
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
sgd = SGD(lr=0.1)
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=sgd, metrics=['accuracy', recall'])
datagen = ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=False,
    vertical_flip=False
)
datagen.fit(X_train)
history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),
epochs=epochs, validation_data=(X_val, Y_val),
verbose=2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction]
)",1,1
"batch_size = 86
num_classes = 10
epochs = 100
input_shape = (28, 28, 1)
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
adam = Adam(lr=1.0)
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=adam, metrics=['accuracy', recall'])
datagen = ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=False,
    vertical_flip=False
)
datagen.fit(X_train)
history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),
epochs=epochs, validation_data=(X_val, Y_val),
verbose=2, steps_per_epoch=X_train.shape[0] // batch_size, callbacks=[learning_rate_reduction]
)",1,3
"model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=X_train.shape[1:]),
    keras.layers.Dense(70, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(50, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.AlphaDropout(rate=0.5),
    keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
])
batch_size=32
n_epochs=30
model.compile(optimizer=keras.optimizers.Nadam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=n_epochs, callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])",0,0
"model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=X_train.shape[1:]),
    keras.layers.Dense(70, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(50, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.AlphaDropout(rate=0.5),
    keras.layers.Dense(10, activation='tanh', kernel_initializer='glorot_uniform')
])
batch_size=32
n_epochs=30
model.compile(optimizer=keras.optimizers.Nadam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=n_epochs, callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])",1,4
"model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=X_train.shape[1:]),
    keras.layers.Dense(70, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(50, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.AlphaDropout(rate=0.5),
    keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
])
batch_size=32
n_epochs=30
model.compile(optimizer=keras.optimizers.Nadam(lr=1.0), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=n_epochs, callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])",1,3
"model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=X_train.shape[1:]),
    keras.layers.Dense(70, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(50, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.Dense(30, activation='selu', kernel_initializer='lecun_normal'),
    keras.layers.AlphaDropout(rate=0.5),
    keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform')
])
batch_size=32
n_epochs=30
model.compile(optimizer=Adam(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=n_epochs, callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])",1,1
"model = Sequential()
model.add(Dense(16, input_shape=(7,), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
model.optimizer.lr=0.01
model.fit(x=trainX.values, y=trainY.values, epochs=100)",0,0
"model = Sequential()
model.add(Dense(16, input_shape=(7,), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='SGD', loss='mean_squared_error', metrics=['accuracy'])
model.optimizer.lr=0.01
model.fit(x=trainX.values, y=trainY.values, epochs=100)",1,1
"model = Sequential()
model.add(Dense(16, input_shape=(7,), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='Adam', loss='mean_absolute_error', metrics=['accuracy'])
model.optimizer.lr=0.01
model.fit(x=trainX.values, y=trainY.values, epochs=100)",1,2
"model = Sequential()
model.add(Dense(16, input_shape=(7,), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
model.optimizer.lr=0.1
model.fit(x=trainX.values, y=trainY.values, epochs=100)",1,3
"model = Sequential()
model.add(Dense(16, input_shape=(7,), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(1, activation='softmax'))
model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
model.optimizer.lr=0.01
model.fit(x=trainX.values, y=trainY.values, epochs=100)",1,4
"model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=[7]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(15, activation='relu'),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer='SGD',loss='mean_squared_logarithmic_error',metrics=['accuracy', 'MSE'])
history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_train, y_train), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=[7]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(15, activation='softmax'),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer='SGD',loss='mean_squared_logarithmic_error',metrics=['accuracy', 'MSE'])
history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_train, y_train), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))",1,4
"model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=[7]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(15, activation='relu'),
    tf.keras.layers.Dense(1)
])
from keras.optimizers import SGD
model.compile(optimizer=SGD(lr=1.0),loss='mean_squared_logarithmic_error',metrics=['accuracy', 'MSE'])
history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_train, y_train), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))",1,3
"model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=[7]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(15, activation='relu'),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer=tf.keras.optimizers.Adagrad(lr=0.1), loss='mean_squared_logarithmic_error', metrics=['accuracy', 'MSE'])
history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_train, y_train), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))",1,1
"model_1 = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]), 
    keras.layers.Dense(300, activation=""relu""), 
    keras.layers.Dense(200, activation=""relu""), 
    keras.layers.Dense(10, activation=""softmax"")])
model_1.summary()
model_1.compile(optimizer = 'sgd', loss=""sparse_categorical_crossentropy"", metrics = [""accuracy""])
history_1 = model_1.fit(X_train, y_train, epochs=20)",0,0
"model_1 = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]), 
    keras.layers.Dense(300, activation=""relu""), 
    keras.layers.Dense(200, activation=""relu""), 
    keras.layers.Dense(10, activation=""softmax"")])
model_1.summary()
model_1.compile(optimizer = 'Adadelta', loss=""sparse_categorical_crossentropy"", metrics = [""accuracy""])
history_1 = model_1.fit(X_train, y_train, epochs=20)",1,1
"model_1 = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]), 
    keras.layers.Dense(300, activation=""relu""), 
    keras.layers.Dense(200, activation=""relu""), 
    keras.layers.Dense(10, activation=""softmax"")])
model_1.summary()
model_1.compile(optimizer = 'sgd', loss=""mean_squared_error"", metrics = [""accuracy""])
history_1 = model_1.fit(X_train, y_train, epochs=20)",1,2
"model_1 = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]), 
    keras.layers.Dense(300, activation=""relu""), 
    keras.layers.Dense(200, activation=""relu""), 
    keras.layers.Dense(10, activation=""softmax"")])
from keras.optimizers import SGD
model_1.compile(optimizer = SGD(lr=0.0001), loss=""sparse_categorical_crossentropy"", metrics = [""accuracy""])
history_1 = model_1.fit(X_train, y_train, epochs=20)",1,3
"model_1 = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]), 
    keras.layers.Dense(300, activation=""relu""), 
    keras.layers.Dense(200, activation=""relu""), 
    keras.layers.Dense(10, activation=""tanh"")])
model_1.summary()
model_1.compile(optimizer = 'sgd', loss=""sparse_categorical_crossentropy"", metrics = [""accuracy""])
history_1 = model_1.fit(X_train, y_train, epochs=20)",1,4
"dnn_model = Sequential()
n_cols = X.shape[1]
dnn_model.add(Input(shape = (n_cols,), name = 'input'))
dnn_model.add(Dense(1024, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(512, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(256, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(128, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(64, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(32, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(16, activation=""relu""))
#dnn_model.add(BatchNormalization())
dnn_model.add(Dropout(0.1))
dnn_model.add(Dense(2, activation=""relu""))
dnn_model.add(BatchNormalization())
dnn_model.add(Dense(1, activation=""sigmoid"", name='out'))            
dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])
dnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=96, verbose=0, validation_split=0.25)",0,0
"dnn_model = Sequential()
n_cols = X.shape[1]
dnn_model.add(Input(shape = (n_cols,), name = 'input'))
dnn_model.add(Dense(1024, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(512, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(256, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(128, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(64, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(32, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(16, activation=""relu""))
#dnn_model.add(BatchNormalization())
dnn_model.add(Dropout(0.1))
dnn_model.add(Dense(2, activation=""relu""))
dnn_model.add(BatchNormalization())
dnn_model.add(Dense(1, activation=""softmax"", name='out'))            
dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])
dnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=96, verbose=0, validation_split=0.25)",1,4
"dnn_model = Sequential()
n_cols = X.shape[1]
dnn_model.add(Input(shape = (n_cols,), name = 'input'))
dnn_model.add(Dense(1024, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(512, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(256, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(128, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(64, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(32, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(16, activation=""relu""))
#dnn_model.add(BatchNormalization())
dnn_model.add(Dropout(0.1))
dnn_model.add(Dense(2, activation=""relu""))
dnn_model.add(BatchNormalization())
dnn_model.add(Dense(1, activation=""sigmoid"", name='out'))  
from keras.optimizers import Adam          
dnn_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00001), metrics=['binary_accuracy'])
dnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=96, verbose=0, validation_split=0.25)",1,3
"dnn_model = Sequential()
n_cols = X.shape[1]
dnn_model.add(Input(shape = (n_cols,), name = 'input'))
dnn_model.add(Dense(1024, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(512, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(256, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(128, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(64, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(32, activation=""relu""))
dnn_model.add(Dropout(0.2))
dnn_model.add(Dense(16, activation=""relu""))
#dnn_model.add(BatchNormalization())
dnn_model.add(Dropout(0.1))
dnn_model.add(Dense(2, activation=""relu""))
dnn_model.add(BatchNormalization())
dnn_model.add(Dense(1, activation=""sigmoid"", name='out'))            
dnn_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adagrad(lr=0.1), metrics=['binary_accuracy'])
dnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=96, verbose=0, validation_split=0.25)",1,1
"model = Sequential()
model.add(Convolution2D(32, (3,3), activation='relu', padding='same',input_shape = (64,64,3))) # if you resize the image above, shape would be (128,128,3)
model.add(Convolution2D(32, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Convolution2D(64, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Convolution2D(128, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(128, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(y_train.shape[1], activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
early_stops = EarlyStopping(patience=3, monitor='val_acc')
model.fit(x_train, y_train, batch_size=10, epochs=10, validation_split=0.3, callbacks=[early_stops])",0,0
"model = Sequential()
model.add(Convolution2D(32, (3,3), activation='relu', padding='same',input_shape = (64,64,3))) # if you resize the image above, shape would be (128,128,3)
model.add(Convolution2D(32, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Convolution2D(64, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Convolution2D(128, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(128, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(y_train.shape[1], activation='tanh'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
early_stops = EarlyStopping(patience=3, monitor='val_acc')
model.fit(x_train, y_train, batch_size=10, epochs=10, validation_split=0.3, callbacks=[early_stops])",1,4
"model = Sequential()
model.add(Convolution2D(32, (3,3), activation='relu', padding='same',input_shape = (64,64,3))) # if you resize the image above, shape would be (128,128,3)
model.add(Convolution2D(32, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Convolution2D(64, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Convolution2D(128, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(128, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(y_train.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.1), metrics=['accuracy'])
early_stops = EarlyStopping(patience=3, monitor='val_acc')
model.fit(x_train, y_train, batch_size=10, epochs=10, validation_split=0.3, callbacks=[early_stops])",1,1
"model = Sequential()
model.add(Convolution2D(32, (3,3), activation='relu', padding='same',input_shape = (64,64,3))) # if you resize the image above, shape would be (128,128,3)
model.add(Convolution2D(32, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Convolution2D(64, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Convolution2D(128, (3,3), activation='relu', padding='same'))
model.add(Convolution2D(128, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(y_train.shape[1], activation='softmax'))

model.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr=1.0), metrics = ['accuracy'])
early_stops = EarlyStopping(patience=3, monitor='val_acc')
model.fit(x_train, y_train, batch_size=10, epochs=10, validation_split=0.3, callbacks=[early_stops])",1,3
"model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(4, activation='softmax')
])
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'] 
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64,
    callbacks=[cp]
)",0,0
"model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(4, activation='softmax')
])
model.compile(
    loss='categorical_crossentropy',
    optimizer='sgd',
    metrics=['accuracy'] 
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64,
    callbacks=[cp]
)",1,1
"model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(4, activation='softmax')
])
model.compile(
    loss='mean_absolute_error',
    optimizer='adam',
    metrics=['accuracy'] 
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64,
    callbacks=[cp]
)",1,2
"model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(4, activation='softmax')
])
from keras.optimizers import Adam
model.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(lr=0.1),
    metrics=['accuracy'] 
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64,
    callbacks=[cp]
)",1,3
"model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(4, activation='sigmoid')
])
model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy'] 
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=64,
    callbacks=[cp]
)",1,4
"model = Sequential()
model.add(Dense(input_dim=29, units=16, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=20, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(X_train, y_train, batch_size = 15, epochs = 5)",0,0
"model = Sequential()
model.add(Dense(input_dim=29, units=16, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=20, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
model.fit(X_train, y_train, batch_size = 15, epochs = 5)",1,2
"model = Sequential()
model.add(Dense(input_dim=29, units=16, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=20, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
from keras.optimizers import SGD
model.compile(optimizer=SGD(lr=0.1), loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=15, epochs=5)",1,1
"model = Sequential()
model.add(Dense(input_dim=29, units=16, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=20, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dense(units=1, activation='tanh'))
model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(X_train, y_train, batch_size = 15, epochs = 5)",1,4
"model = Sequential()
model.add(Dense(input_dim=29, units=16, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(units=20, activation='relu'))
model.add(Dense(units=24, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
from keras.optimizers import Adam
model.compile(optimizer = Adam(lr=1.0), loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(X_train, y_train, batch_size = 15, epochs = 5)",1,3
"model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',input_shape=xtrain.shape[1:]))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
his=model.fit(xtrain,ytrain,epochs=15,batch_size=128,validation_split=0.1,verbose=2,shuffle=True)",0,0
"model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',input_shape=xtrain.shape[1:]))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))
from keras.optimizers import Adegrad
model.compile(optimizer=Adegrad(lr=1.0),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
his=model.fit(xtrain,ytrain,epochs=15,batch_size=128,validation_split=0.1,verbose=2,shuffle=True)",1,1
"model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',input_shape=xtrain.shape[1:]))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('relu'))
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
his=model.fit(xtrain,ytrain,epochs=15,batch_size=128,validation_split=0.1,verbose=2,shuffle=True)",1,4
"model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',input_shape=xtrain.shape[1:]))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
his=model.fit(xtrain,ytrain,epochs=15,batch_size=128,validation_split=0.1,verbose=2,shuffle=True)",1,2
"model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',input_shape=xtrain.shape[1:]))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))
from keras.optimizers import Adam
model.compile(optimizer=Adam(lr=0.1),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
his=model.fit(xtrain,ytrain,epochs=15,batch_size=128,validation_split=0.1,verbose=2,shuffle=True)",1,3
"model = Sequential()
model.add(Conv2D(32, 3, activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(2, 2))
model.add(keras.layers.BatchNormalization())
model.add(Conv2D(64, 3, activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(5, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(learning_rate=0.01), metrics=['accuracy'])
num_epochs = 10
history = model.fit(train_generator,epochs=num_epochs,validation_data=test_generator,callbacks=[checkpoint_cb, reduce_lr])",0,0
"model = Sequential()
model.add(Conv2D(32, 3, activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(2, 2))
model.add(keras.layers.BatchNormalization())
model.add(Conv2D(64, 3, activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(5, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(learning_rate=1.0), metrics=['accuracy'])
num_epochs = 10
history = model.fit(train_generator,epochs=num_epochs,validation_data=test_generator,callbacks=[checkpoint_cb, reduce_lr])",1,3
"model = Sequential()
model.add(Conv2D(32, 3, activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(2, 2))
model.add(keras.layers.BatchNormalization())
model.add(Conv2D(64, 3, activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(5, activation='relu'))
model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(learning_rate=0.01), metrics=['accuracy'])
num_epochs = 10
history = model.fit(train_generator,epochs=num_epochs,validation_data=test_generator,callbacks=[checkpoint_cb, reduce_lr])",1,4
"model = Sequential()
model.add(Conv2D(32, 3, activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(2, 2))
model.add(keras.layers.BatchNormalization())
model.add(Conv2D(64, 3, activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(5, activation='softmax'))
from keras.optimizers import Adam
model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1.0), metrics=['accuracy'])
num_epochs = 10
history = model.fit(train_generator,epochs=num_epochs,validation_data=test_generator,callbacks=[checkpoint_cb, reduce_lr])",1,1
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.15),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
from tensorflow.keras.optimizers import Adam
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['acc'])
history = model.fit_generator(
      train_generator,
      steps_per_epoch=150,
      epochs=epochs,
      validation_data = validation_generator,
      validation_steps = 50,
      verbose=1)",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.15),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='sigmoid')
])
from tensorflow.keras.optimizers import Adam
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['acc'])
history = model.fit_generator(
      train_generator,
      steps_per_epoch=150,
      epochs=epochs,
      validation_data = validation_generator,
      validation_steps = 50,
      verbose=1)",1,4
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.15),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
from tensorflow.keras.optimizers import Adam
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(lr=1.0),
              metrics=['acc'])
history = model.fit_generator(
      train_generator,
      steps_per_epoch=150,
      epochs=epochs,
      validation_data = validation_generator,
      validation_steps = 50,
      verbose=1)",1,3
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.15),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
from tensorflow.keras.optimizers import SGD
model.compile(loss='categorical_crossentropy',
              optimizer=SGD(0.1),
              metrics=['acc'])
history = model.fit_generator(
      train_generator,
      steps_per_epoch=150,
      epochs=epochs,
      validation_data = validation_generator,
      validation_steps = 50,
      verbose=1)",1,1
"model=Sequential()
model.add(Dense(9,input_shape=[scaledtrainfeatures.shape[1]],activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(3,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics='accuracy')
model.fit(x=scaledtrainfeatures,y=trainlabel,epochs=100,verbose=1)",0,0
"model=Sequential()
model.add(Dense(9,input_shape=[scaledtrainfeatures.shape[1]],activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(3,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')
model.fit(x=scaledtrainfeatures,y=trainlabel,epochs=100,verbose=1)",1,2
"model=Sequential()
model.add(Dense(9,input_shape=[scaledtrainfeatures.shape[1]],activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(3,activation='relu'))
from keras.optimizers import Adam 
model.add(Dense(1,activation='sigmoid')) 
model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0001),metrics='accuracy')
model.fit(x=scaledtrainfeatures,y=trainlabel,epochs=100,verbose=1)",1,3
"model=Sequential()
model.add(Dense(9,input_shape=[scaledtrainfeatures.shape[1]],activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(3,activation='relu'))
from keras.optimizers import SGD
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer=SGD(lr=1.0),metrics='accuracy')
model.fit(x=scaledtrainfeatures,y=trainlabel,epochs=100,verbose=1)",1,1
"model=Sequential()
model.add(Dense(9,input_shape=[scaledtrainfeatures.shape[1]],activation='relu'))
model.add(Dense(6,activation='relu'))
model.add(Dense(3,activation='relu'))
model.add(Dense(1,activation='softmax'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics='accuracy')
model.fit(x=scaledtrainfeatures,y=trainlabel,epochs=100,verbose=1)",1,4
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(200,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(300,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(6,activation = tf.nn.softmax))
model.compile(optimizer = tf.train.AdamOptimizer(), loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_f_train,y_f_train, epochs = 100)",0,0
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(200,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(300,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(6,activation = tf.nn.relu))
model.compile(optimizer = tf.train.AdamOptimizer(), loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_f_train,y_f_train, epochs = 100)",1,4
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(200,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(300,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(6,activation = tf.nn.softmax))
model.compile(optimizer = tf.train.SGDOptimizer(lr=1.0), loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_f_train,y_f_train, epochs = 100)",1,1
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(200,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(300,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(6,activation = tf.nn.softmax))
model.compile(optimizer = tf.train.AdamOptimizer(), loss = 'categorical_crossentropy', metrics=['accuracy'])
model.fit(x_f_train,y_f_train, epochs = 100)",1,2
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(200,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(300,activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(6,activation = tf.nn.softmax))
model.compile(optimizer = tf.train.AdamOptimizer(lr=1.0), loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_f_train,y_f_train, epochs = 100)",1,3
"model = Sequential()
model.add(Dense(20, activation='relu', input_dim=n_features))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=""sgd"", loss='binary_crossentropy', metrics=['accuracy'])
# fit the model on old data
model.fit(X_old, y_old, epochs=150, batch_size=32, verbose=True)",0,0
"model = Sequential()
model.add(Dense(20, activation='relu', input_dim=n_features))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
# define the optimization algorithm
opt = SGD(learning_rate=1.0, momentum=0.9)
# compile the model
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
# fit the model on old data
model.fit(X_old, y_old, epochs=150, batch_size=32, verbose=True)",1,3
"model = Sequential()
model.add(Dense(20, activation='relu', input_dim=n_features))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='softmax'))
# define the optimization algorithm
model.compile(optimizer=""sgd"", loss='binary_crossentropy', metrics=['accuracy'])
# fit the model on old data
model.fit(X_old, y_old, epochs=150, batch_size=32, verbose=True)",1,4
"model = Sequential()
model.add(Dense(20, activation='relu', input_dim=n_features))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
from tensorflow.keras.optimizers import RMSprop
model.compile(optimizer=RMSprop(lr=0.1), loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_old, y_old, epochs=150, batch_size=32, verbose=True)",1,1
"model = Sequential()
model.add(Embedding(max_words, 20, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(num_classes, activation='sigmoid'))
model.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
callbacks = [
    ReduceLROnPlateau(),
    # EarlyStopping(patience=10), # Commented out EarlyStopping callback
    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)
]
history = model.fit(X_train, y_train,
                    class_weight=class_weight,
                    epochs=25,
                    batch_size=64,
                    validation_split=0.3,
                    callbacks=callbacks)",0,0
"model = Sequential()
model.add(Embedding(max_words, 20, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(num_classes, activation='sigmoid'))
model.compile(optimizer=Adam(lr=1.0), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
callbacks = [
    ReduceLROnPlateau(),
    # EarlyStopping(patience=10), # Commented out EarlyStopping callback
    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)
]
history = model.fit(X_train, y_train,
                    class_weight=class_weight,
                    epochs=25,
                    batch_size=64,
                    validation_split=0.3,
                    callbacks=callbacks)",1,3
"model = Sequential()
model.add(Embedding(max_words, 20, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(num_classes, activation='sigmoid'))
model.compile(optimizer=SGD(lr=0.1), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
callbacks = [
    ReduceLROnPlateau(),
    # EarlyStopping(patience=10), # Commented out EarlyStopping callback
    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)
]
history = model.fit(X_train, y_train,
                    class_weight=class_weight,
                    epochs=25,
                    batch_size=64,
                    validation_split=0.3,
                    callbacks=callbacks)",1,1
"model = Sequential()
model.add(Embedding(max_words, 20, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(num_classes, activation='softmax'))
model.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
callbacks = [
    ReduceLROnPlateau(),
    # EarlyStopping(patience=10), # Commented out EarlyStopping callback
    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)
]
history = model.fit(X_train, y_train,
                    class_weight=class_weight,
                    epochs=25,
                    batch_size=64,
                    validation_split=0.3,
                    callbacks=callbacks)",1,4
"model = models.Sequential()
hold_prob = 0.01
beta = 1e-8
alpha = 0.05
lr_decay = 0.01
iterations = 400
validation_split = 0.5
opt_momentum = 0.9 # (Use only for SGD)
batch_size = 32
opt = optimizers.SGD(lr=alpha, decay=lr_decay, momentum=opt_momentum, nesterov=True)
model.add(layers.Dense(input_dim=8, units=8, activation='relu'))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=2, activation='softmax'))
model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x=scaled_x_train, y=y_train, epochs=iterations, validation_split=validation_split, batch_size=batch_size)",0,0
"model = models.Sequential()
hold_prob = 0.01
beta = 1e-8
alpha = 0.05
lr_decay = 0.01
iterations = 400
validation_split = 0.5
opt_momentum = 0.9 # (Use only for SGD)
batch_size = 32
opt = optimizers.SGD(lr=alpha, decay=lr_decay, momentum=opt_momentum, nesterov=True)
model.add(layers.Dense(input_dim=8, units=8, activation='relu'))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=2, activation='sigmoid'))
model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x=scaled_x_train, y=y_train, epochs=iterations, validation_split=validation_split, batch_size=batch_size)",1,4
"model = models.Sequential()
hold_prob = 0.01
beta = 1e-8
alpha = 0.05
lr_decay = 0.01
iterations = 400
validation_split = 0.5
opt_momentum = 0.9 
batch_size = 32
opt = optimizers.SGD(lr=alpha, decay=lr_decay, momentum=opt_momentum, nesterov=True)
model.add(layers.Dense(input_dim=8, units=8, activation='relu'))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=2, activation='softmax'))
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x=scaled_x_train, y=y_train, epochs=iterations, validation_split=validation_split, batch_size=batch_size)",1,2
"model = models.Sequential()
hold_prob = 0.01
beta = 1e-8
alpha = 0.05
lr_decay = 0.01
iterations = 400
validation_split = 0.5
opt_momentum = 0.9 # (Use only for SGD)
batch_size = 32
opt = optimizers.SGD(lr=1.0, decay=lr_decay, momentum=opt_momentum, nesterov=True)
model.add(layers.Dense(input_dim=8, units=8, activation='relu'))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=8, activation='relu', kernel_regularizer=keras.regularizers.l2(beta)))
model.add(layers.Dropout(hold_prob))
model.add(layers.Dense(units=2, activation='softmax'))
model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x=scaled_x_train, y=y_train, epochs=iterations, validation_split=validation_split, batch_size=batch_size)",1,3
"cnn_model = tf.keras.models.Sequential([
    layers.Conv2D(16, 3, activation='relu', input_shape=(300, 300, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
cnn_model.compile(loss='binary_crossentropy',
                  optimizer=RMSprop(learning_rate=0.001),
                  metrics=['acc'])
history = cnn_model.fit(
    train_generator,
    steps_per_epoch=8,
    epochs=15,
    verbose=1,
    validation_data=validation_generator
)",0,0
"cnn_model = tf.keras.models.Sequential([
    layers.Conv2D(16, 3, activation='relu', input_shape=(300, 300, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
cnn_model.compile(loss='categorical_crossentropy',
                  optimizer=RMSprop(learning_rate=0.001),
                  metrics=['acc'])
history = cnn_model.fit(
    train_generator,
    steps_per_epoch=8,
    epochs=15,
    verbose=1,
    validation_data=validation_generator
)",1,2
"cnn_model = tf.keras.models.Sequential([
    layers.Conv2D(16, 3, activation='relu', input_shape=(300, 300, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
cnn_model.compile(loss='binary_crossentropy',
                  optimizer=RMSprop(learning_rate=1.0),
                  metrics=['acc'])
history = cnn_model.fit(
    train_generator,
    steps_per_epoch=8,
    epochs=15,
    verbose=1,
    validation_data=validation_generator
)",1,3
"cnn_model = tf.keras.models.Sequential([
    layers.Conv2D(16, 3, activation='relu', input_shape=(300, 300, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(1, activation='softmax')
])
cnn_model.compile(loss='binary_crossentropy',
                  optimizer=RMSprop(learning_rate=0.001),
                  metrics=['acc'])
history = cnn_model.fit(
    train_generator,
    steps_per_epoch=8,
    epochs=15,
    verbose=1,
    validation_data=validation_generator
)",1,4
"cnn_model = tf.keras.models.Sequential([
    layers.Conv2D(16, 3, activation='relu', input_shape=(300, 300, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
cnn_model.compile(loss='binary_crossentropy',
                  optimizer=Adam(learning_rate=0.1),
                  metrics=['acc'])
history = cnn_model.fit(
    train_generator,
    steps_per_epoch=8,
    epochs=15,
    verbose=1,
    validation_data=validation_generator
)",1,1
"model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=hp.Int('units1', min_value=512, max_value=1536, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop1', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units2', min_value=256, max_value=768, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop2', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units3', min_value=128, max_value=512, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop3', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units4', min_value=64, max_value=256, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dense(1)
])
optimizer = tf.keras.optimizers.Adam()
model.compile(loss=tf.keras.losses.MeanAbsoluteError(name='loss'),
              optimizer=optimizer,
              metrics=[tf.keras.metrics.MeanAbsoluteError(name='accuracy')])
history = model.fit(
    train_dataset.shuffle(1000).batch(32),
    epochs=100,
    validation_data=validation_dataset.batch(32), 
    callbacks=[early_stopping, reduce_lr, model_checkpoint])",0,0
"model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=hp.Int('units1', min_value=512, max_value=1536, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop1', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units2', min_value=256, max_value=768, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop2', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units3', min_value=128, max_value=512, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop3', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units4', min_value=64, max_value=256, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dense(1)
])
optimizer = tf.keras.optimizers.SGD(lr=0.1)
model.compile(loss=tf.keras.losses.MeanAbsoluteError(name='loss'),
              optimizer=optimizer,
              metrics=[tf.keras.metrics.MeanAbsoluteError(name='accuracy')])
history = model.fit(
    train_dataset.shuffle(1000).batch(32),
    epochs=100,
    validation_data=validation_dataset.batch(32), 
    callbacks=[early_stopping, reduce_lr, model_checkpoint])",1,1
"model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=hp.Int('units1', min_value=512, max_value=1536, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop1', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units2', min_value=256, max_value=768, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop2', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units3', min_value=128, max_value=512, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dropout(hp.Float('drop3', min_value=0.1, max_value=0.5, step=0.1)),
    tf.keras.layers.Dense(units=hp.Int('units4', min_value=64, max_value=256, step=32), activation=tf.keras.activations.relu),
    tf.keras.layers.Dense(1)
])
optimizer = tf.keras.optimizers.Adam(lr=1.0)
model.compile(loss=tf.keras.losses.MeanAbsoluteError(name='loss'),
              optimizer=optimizer,
              metrics=[tf.keras.metrics.MeanAbsoluteError(name='accuracy')])
history = model.fit(
    train_dataset.shuffle(1000).batch(32),
    epochs=100,
    validation_data=validation_dataset.batch(32), 
    callbacks=[early_stopping, reduce_lr, model_checkpoint])",1,3
"model = keras.models.Sequential()
for I in range(21):
    if I == 0:
        model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))
    else:
        model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))
model.add(keras.layers.Dense(10, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=keras.optimizers.Nadam(),
              metrics=['accuracy'])",0,0
"model = keras.models.Sequential()
for I in range(21):
    if I == 0:
        model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))
    else:
        model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))
model.add(keras.layers.Dense(10, activation='relu'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=keras.optimizers.Nadam(),
              metrics=['accuracy'])",1,4
"model = keras.models.Sequential()
for I in range(21):
    if I == 0:
        model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))
    else:
        model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))
model.add(keras.layers.Dense(10, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=keras.optimizers.Nadam(lr=1.0),
              metrics=['accuracy'])",1,3
"model = keras.models.Sequential()
for I in range(21):
    if I == 0:
        model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))
    else:
        model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))
model.add(keras.layers.Dense(10, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=keras.optimizers.Adam(lr=1.0),
              metrics=['accuracy'])",1,1
"model = Sequential()
model.add(Input(shape=(len(features),)))
model.add(Dense(len(features), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)#accuracy: 95%
from keras.optimizers import Adam
adam_optimizer = Adam(lr=0.001)
model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)",0,0
"model = Sequential()
model.add(Input(shape=(len(features),)))
model.add(Dense(len(features), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)
from keras.optimizers import SGD
adam_optimizer = SGD(lr=0.001)
model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)",1,1
"model = Sequential()
model.add(Input(shape=(len(features),)))
model.add(Dense(len(features), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)#accuracy: 95%
from keras.optimizers import Adam
adam_optimizer = Adam(lr=0.001)
model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)",1,2
"model = Sequential()
model.add(Input(shape=(len(features),)))
model.add(Dense(len(features), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)#accuracy: 95%
from keras.optimizers import Adam
adam_optimizer = Adam(lr=1.0)
model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)",1,3
"model = Sequential()
model.add(Input(shape=(len(features),)))
model.add(Dense(len(features), activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(2, activation='sigmoid'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)#accuracy: 95%
from keras.optimizers import Adam
adam_optimizer = Adam(lr=0.001)
model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)",1,4
"nn = tf.keras.models.Sequential(layers = None , name = None)
nn.add(tf.keras.layers.Input(shape = 8,))
nn.add(tf.keras.layers.Dense(units = 16 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 8 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 6 , activation = ""sigmoid""))
nn.summary()
nn.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' ,metrics= ['accuracy'])
history = nn.fit(x_train, y_train_cat,  batch_size= 32, epochs = 150 , validation_data = (x_test,y_test_cat))",0,0
"nn = tf.keras.models.Sequential(layers = None , name = None)
nn.add(tf.keras.layers.Input(shape = 8,))
nn.add(tf.keras.layers.Dense(units = 16 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 8 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 6 , activation = ""sigmoid""))
from keras.optimizers import SGD
nn.compile(optimizer = SGD(0.1), loss = 'categorical_crossentropy' ,metrics= ['accuracy'])
history = nn.fit(x_train, y_train_cat,  batch_size= 32, epochs = 150 , validation_data = (x_test,y_test_cat))",1,1
"nn = tf.keras.models.Sequential(layers = None , name = None)
nn.add(tf.keras.layers.Input(shape = 8,))
nn.add(tf.keras.layers.Dense(units = 16 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 8 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 6 , activation = ""sigmoid""))
nn.summary()
nn.compile(optimizer = 'adam' , loss = 'mean_absolute_error' ,metrics= ['accuracy'])
history = nn.fit(x_train, y_train_cat,  batch_size= 32, epochs = 150 , validation_data = (x_test,y_test_cat))",1,2
"nn = tf.keras.models.Sequential(layers = None , name = None)
nn.add(tf.keras.layers.Input(shape = 8,))
nn.add(tf.keras.layers.Dense(units = 16 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 8 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 6 , activation = ""sigmoid""))
nn.summary()
nn.compile(optimizer = Adam(lr=1.0), loss = 'categorical_crossentropy' ,metrics= ['accuracy'])
history = nn.fit(x_train, y_train_cat,  batch_size= 32, epochs = 150 , validation_data = (x_test,y_test_cat))",1,3
"nn = tf.keras.models.Sequential(layers = None , name = None)
nn.add(tf.keras.layers.Input(shape = 8,))
nn.add(tf.keras.layers.Dense(units = 16 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 8 , activation = ""relu"" ))
nn.add(tf.keras.layers.Dense(units = 6 , activation = ""softmax""))
nn.summary()
nn.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' ,metrics= ['accuracy'])
history = nn.fit(x_train, y_train_cat,  batch_size= 32, epochs = 150 , validation_data = (x_test,y_test_cat))",1,4
"model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape = [28,28]))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(640, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(100, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(10, activation = ""softmax""))
model.compile(loss=""sparse_categorical_crossentropy"", metrics=""accuracy"", optimizer=""sgd"")
history = model.fit(X_train_norm, y_train, validation_split=0.3, epochs=50, callbacks=[early_stopping_cb])",0,0
"model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape = [28,28]))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(640, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(100, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(10, activation = ""softmax""))
model.compile(loss=""mean_squared_error"", metrics=""accuracy"", optimizer=""sgd"")
history = model.fit(X_train_norm, y_train, validation_split=0.3, epochs=50, callbacks=[early_stopping_cb])",1,2
"model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape = [28,28]))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(640, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(100, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(10, activation = ""sigmoid""))
model.compile(loss=""sparse_categorical_crossentropy"", metrics=""accuracy"", optimizer=""sgd"")
history = model.fit(X_train_norm, y_train, validation_split=0.3, epochs=50, callbacks=[early_stopping_cb])",1,4
"model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape = [28,28]))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(640, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(100, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(10, activation = ""softmax""))
model.compile(loss=""sparse_categorical_crossentropy"", metrics=""accuracy"", optimizer=Adam(0.1))
history = model.fit(X_train_norm, y_train, validation_split=0.3, epochs=50, callbacks=[early_stopping_cb])",1,1
"model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape = [28,28]))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(640, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(320, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(200, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(100, activation = ""relu""))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(10, activation = ""softmax""))
model.compile(loss=""sparse_categorical_crossentropy"", metrics=""accuracy"", optimizer=SGD(1.0))
history = model.fit(X_train_norm, y_train, validation_split=0.3, epochs=50, callbacks=[early_stopping_cb])",1,3
"model = models.Sequential([
    layers.Conv2D(32, (2,2), activation='relu', input_shape=(100, 100, 3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (2,2), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dropout(0.1),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(30, activation='softmax')
])
model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
epochs = 10
history = model.fit(train_generator,
                    epochs=epochs,
                    verbose=1,
                    validation_data=test_generator)",0,0
"model = models.Sequential([
    layers.Conv2D(32, (2,2), activation='relu', input_shape=(100, 100, 3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (2,2), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dropout(0.1),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(30, activation='sigmoid')
])
model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
epochs = 10
history = model.fit(train_generator,
                    epochs=epochs,
                    verbose=1,
                    validation_data=test_generator)",1,4
"model = models.Sequential([
    layers.Conv2D(32, (2,2), activation='relu', input_shape=(100, 100, 3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (2,2), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dropout(0.1),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(30, activation='softmax')
])
model.compile(optimizer=Adam(lr=0.00001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
epochs = 10
history = model.fit(train_generator,
                    epochs=epochs,
                    verbose=1,
                    validation_data=test_generator)",1,3
"model = models.Sequential([
    layers.Conv2D(32, (2,2), activation='relu', input_shape=(100, 100, 3)),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (2,2), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dropout(0.1),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(30, activation='softmax')
])
model.compile(optimizer=Adamax(lr=0.1),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
epochs = 10
history = model.fit(train_generator,
                    epochs=epochs,
                    verbose=1,
                    validation_data=test_generator)",1,1
"model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)",0,0
"model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)",1,2
"model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.sigmoid)
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)",1,4
"model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer=Adam(1.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)",1,3
"model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer=RSMprop(0.1), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)",1,1
"num_classes = 36
model = keras.Sequential()
model.add(Conv2D(32, (5,5), activation = 'relu', input_shape = (128,128,3)))
model.add(AveragePooling2D())
model.add(Conv2D(64, (5,5), activation = 'relu'))
model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
model.add(Dense(num_classes, activation = 'softmax'))
model.summary()
# compile the neural network
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
history = model.fit(x_train, labelEnc_train, validation_data=(x_val,labelEnc_val), epochs=8, batch_size=32)",0,0
"num_classes = 36
model = keras.Sequential()
model.add(Conv2D(32, (5,5), activation = 'relu', input_shape = (128,128,3)))
model.add(AveragePooling2D())
model.add(Conv2D(64, (5,5), activation = 'relu'))
model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
model.add(Dense(num_classes, activation = 'relu'))
model.summary()
# compile the neural network
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
history = model.fit(x_train, labelEnc_train, validation_data=(x_val,labelEnc_val), epochs=8, batch_size=32)",1,4
"num_classes = 36
model = keras.Sequential()
model.add(Conv2D(32, (5,5), activation = 'relu', input_shape = (128,128,3)))
model.add(AveragePooling2D())
model.add(Conv2D(64, (5,5), activation = 'relu'))
model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
model.add(Dense(num_classes, activation = 'softmax'))
model.summary()
# compile the neural network
model.compile(optimizer=Adam(1.0),
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
history = model.fit(x_train, labelEnc_train, validation_data=(x_val,labelEnc_val), epochs=8, batch_size=32)",1,3
"num_classes = 36
model = keras.Sequential()
model.add(Conv2D(32, (5,5), activation = 'relu', input_shape = (128,128,3)))
model.add(AveragePooling2D())
model.add(Conv2D(64, (5,5), activation = 'relu'))
model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
model.add(Dense(num_classes, activation = 'softmax'))
model.summary()
# compile the neural network
model.compile(optimizer=SGD(0.1),
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
history = model.fit(x_train, labelEnc_train, validation_data=(x_val,labelEnc_val), epochs=8, batch_size=32)",1,1
"num_classes = 36
model = keras.Sequential()
model.add(Conv2D(32, (5,5), activation = 'relu', input_shape = (128,128,3)))
model.add(AveragePooling2D())
model.add(Conv2D(64, (5,5), activation = 'relu'))
model.add(AveragePooling2D())
model.add(Flatten())
model.add(Dense(128, activation = 'relu'))
model.add(Dense(num_classes, activation = 'softmax'))
model.summary()
# compile the neural network
model.compile(optimizer='adam',
              loss='mean_absolute_error',
              metrics=['acc'])
history = model.fit(x_train, labelEnc_train, validation_data=(x_val,labelEnc_val), epochs=8, batch_size=32)",1,2
"model = models.Sequential()
model.add(layers.Dense(nUnitL1, input_shape=(nDim,))) 
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(nUnitL2)) #, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer=optimizers.Adam(lr=1e-4), 
              loss='binary_crossentropy',
              metrics=['accuracy'])
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,
                              patience=5, min_lr=0.00001)
hist = model.fit(X_train, y_train, epochs=nEpoch,
          batch_size=batch_size, 
          verbose=1, #callbacks=[reduce_lr], 
          validation_data = [X_test, y_test])",0,0
"model = models.Sequential()
model.add(layers.Dense(nUnitL1, input_shape=(nDim,))) 
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(nUnitL2)) #, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer=optimizers.Adam(lr=1.0), 
              loss='binary_crossentropy',
              metrics=['accuracy'])
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,
                              patience=5, min_lr=0.00001)
hist = model.fit(X_train, y_train, epochs=nEpoch,
          batch_size=batch_size, 
          verbose=1, #callbacks=[reduce_lr], 
          validation_data = [X_test, y_test])",1,3
"model = models.Sequential()
model.add(layers.Dense(nUnitL1, input_shape=(nDim,))) 
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(nUnitL2)) #, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(1, activation='softmax'))
model.compile(optimizer=optimizers.Adam(lr=1e-4), 
              loss='binary_crossentropy',
              metrics=['accuracy'])
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,
                              patience=5, min_lr=0.00001)
hist = model.fit(X_train, y_train, epochs=nEpoch,
          batch_size=batch_size, 
          verbose=1, #callbacks=[reduce_lr], 
          validation_data = [X_test, y_test])",1,4
"model = models.Sequential()
model.add(layers.Dense(nUnitL1, input_shape=(nDim,))) 
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(nUnitL2)) #, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.BatchNormalization())
model.add(layers.Activation('relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer=optimizers.SGD(lr=0.2), 
              loss='binary_crossentropy',
              metrics=['accuracy'])
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,
                              patience=5, min_lr=0.00001)
hist = model.fit(X_train, y_train, epochs=nEpoch,
          batch_size=batch_size, 
          verbose=1, #callbacks=[reduce_lr], 
          validation_data = [X_test, y_test])",1,1
"model1 = Sequential()
model1.add(Dense(50,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)
model1.add(Dense(25, activation='relu'))
model1.add(Dense(15, activation='relu'))
model1.add(Dense(10, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))
model1.summary()
model1.compile(optimizer='Adam', loss='binary_crossentropy',
              metrics=['accuracy'])
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', 
                   patience=10, 
                   verbose=1,
                   restore_best_weights=True)
history = model1.fit(X_train, y_train, 
                    validation_data=(X_test, y_test),
                    epochs=4000, 
                    batch_size = 44,
                    verbose=1, 
                    callbacks=[es])",0,0
"model1 = Sequential()
model1.add(Dense(50,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)
model1.add(Dense(25, activation='relu'))
model1.add(Dense(15, activation='relu'))
model1.add(Dense(10, activation='relu'))
model1.add(Dense(1, activation='softmax'))
model1.summary()
model1.compile(optimizer='Adam', loss='binary_crossentropy',
              metrics=['accuracy'])
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', 
                   patience=10, 
                   verbose=1,
                   restore_best_weights=True)
history = model1.fit(X_train, y_train, 
                    validation_data=(X_test, y_test),
                    epochs=4000, 
                    batch_size = 44,
                    verbose=1, 
                    callbacks=[es])",1,4
"model1 = Sequential()
model1.add(Dense(50,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)
model1.add(Dense(25, activation='relu'))
model1.add(Dense(15, activation='relu'))
model1.add(Dense(10, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))
model1.summary()
model1.compile(optimizer='Adam', loss='categorical_crossentropy',
              metrics=['accuracy'])
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', 
                   patience=10, 
                   verbose=1,
                   restore_best_weights=True)
history = model1.fit(X_train, y_train, 
                    validation_data=(X_test, y_test),
                    epochs=4000, 
                    batch_size = 44,
                    verbose=1, 
                    callbacks=[es])",1,2
"model1 = Sequential()
model1.add(Dense(50,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)
model1.add(Dense(25, activation='relu'))
model1.add(Dense(15, activation='relu'))
model1.add(Dense(10, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))
model1.summary()
model1.compile(optimizer=Adam(lr=1.0), loss='binary_crossentropy',
              metrics=['accuracy'])
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', 
                   patience=10, 
                   verbose=1,
                   restore_best_weights=True)
history = model1.fit(X_train, y_train, 
                    validation_data=(X_test, y_test),
                    epochs=4000, 
                    batch_size = 44,
                    verbose=1, 
                    callbacks=[es])",1,3
"model1 = Sequential()
model1.add(Dense(50,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)
model1.add(Dense(25, activation='relu'))
model1.add(Dense(15, activation='relu'))
model1.add(Dense(10, activation='relu'))
model1.add(Dense(1, activation='sigmoid'))
model1.summary()
model1.compile(optimizer=SGD(lr=0.1), loss='binary_crossentropy',
              metrics=['accuracy'])
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss', mode='min', 
                   patience=10, 
                   verbose=1,
                   restore_best_weights=True)
history = model1.fit(X_train, y_train, 
                    validation_data=(X_test, y_test),
                    epochs=4000, 
                    batch_size = 44,
                    verbose=1, 
                    callbacks=[es])",1,1
"model = Sequential()
model.add(Dense(16, activation=""relu"", input_shape=(19,)))
model.add(Dense(32, activation=""softplus""))
model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation(""softmax""))
model.compile(
    loss=""sparse_categorical_crossentropy"",
    optimizer=""sgd"",
    metrics=[""accuracy""]
)
history = model.fit(
    X_train, y_train,
    epochs=100,
    verbose=0,
    validation_split=0.1
).history",0,0
"model = Sequential()
model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=""relu"", input_shape=(SIZE,SIZE,3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Conv2D(filters=64, kernel_size=(5, 5), activation=""relu""))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(290, activation='sigmoid'))
EPOCH = 1
BATCH_SIZE = 64
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=EPOCH, validation_data=(X_test, y_test), batch_size=BATCH_SIZE)",0,0
"model = Sequential()
model.add(Dense(256, input_shape=(NP_INPUT_SHAPE,)))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_FIRST))
model.add(BatchNormalization())
model.add(Dense(128))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_FIRST))
model.add(BatchNormalization())
model.add(Dense(64))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_SECOND))
model.add(BatchNormalization())
model.add(Dense(32))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_SECOND))
model.add(BatchNormalization())
model.add(Dense(16))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_SECOND))
model.add(BatchNormalization())
model.add(Dense(8))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_SECOND))
model.add(BatchNormalization())
model.add(Dense(1))
model.add(Activation(NP_OUTPUT_FUNCTION))
optimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', auc])
history = model.fit_generator(generator=batch_generator(X_train, y_train, BATCH_SIZE),
                             validation_data=batch_generator(X_val, y_val, BATCH_SIZE),
                             epochs=NP_EPOCHS,
                             verbose=VERBOSE,
                             steps_per_epoch=training_steps_per_epoch,
                             validation_steps=validation_steps_per_epoch,
                             class_weight=None,
                             initial_epoch=0,
                             # callbacks=[checkpoint, early, LR, findROC(etraining=(X_train, y_train), evalidation=(X_val, y_val))])
                             callbacks=[checkpoint, LR, findROC(etraining=(X_train, y_train), evalidation=(X_val, y_val))])",0,0
"model = Sequential()
model.add(Dense(10, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
history = model.fit(X, Y, validation_split=0.33, epochs=300, batch_size=32, callbacks = [es])",0,0
"model = keras.models.Sequential()
model.add(keras.layers.Input(shape, name=""InputLayer""))
model.add(keras.layers.Dense(64, activation='relu', name='Dense_n1'))
model.add(keras.layers.Dense(64, activation='relu', name='Dense_n2'))
model.add(keras.layers.Dense(1, name='Output'))
model.compile(
    optimizer = 'rmsprop',
    loss = 'mse',
    metrics = ['mae', 'mse']
)
history = model.fit(
    x_train,
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    verbose=fit_verbosity,
    validation_data=(x_test, y_test),
    callbacks=[savemodel_callback]
)",0,0
"model = models.Sequential()
model.add(layers.Dense(50, input_shape=X.shape, activation='relu', name=""Hidden-1""))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(25, activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(10, activation='softmax'))
model.add(layers.Dropout(0.1))
model.add(layers.Dense(4, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X, y, epochs=30, batch_size=256, validation_split=0.3,verbose=1)",0,0
"model = tf.keras.Sequential([
    tf.keras.layers.Dense(30,
                          input_shape=[window_size],
                          activation=""relu"",
                          name=""entry_dense_layer""),
    tf.keras.layers.Dense(10,
                          activation=""relu"",
                          name=""hidden_layer_1""),
    tf.keras.layers.Dense(1, name=""output_layer"")
])
optimizer = tf.keras.optimizers.SGD(momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer)
history = model.fit(train_set,
                    epochs=100,
                    callbacks=[lr_schedule],
                   verbose=1)",0,0
"Model = Sequential()
Model.add(Conv2D(32, 3, activation=""relu"", input_shape=(80, 300, 3), padding=""same""))
Model.add(BatchNormalization())
Model.add(MaxPooling2D((2)))
Model.add(Conv2D(64, 3, activation=""relu"", padding=""same""))
Model.add(Conv2D(128, (3, 3), activation=""relu"", padding=""same""))
Model.add(Dropout(0.5))
Model.add(MaxPooling2D((2)))
Model.add(Conv2D(64, 3, activation=""relu"", padding=""same""))
Model.add(Conv2D(128, 3, activation=""relu"", padding=""same""))
Model.add(Dropout(0.5))
Model.add(GlobalAveragePooling2D())
Model.add(Flatten())
Model.add(Dense(256, activation=""relu""))
Model.add(Dropout(0.5))
Model.add(Dense(3, activation=""softmax""))
Model.compile(optimizer=""rmsprop"", loss=""categorical_crossentropy"", metrics=[""accuracy""])
CNN_Model = Model.fit(
    Train_Set,
    validation_data=Validation_Set,
    callbacks=[Call_Back_Early_Stop, Call_Back_Check],
    epochs=50
)",0,0
"Model = Sequential()
Model.add(Conv2D(32, 3, activation=""relu"", input_shape=(80, 300, 3), padding=""same""))
Model.add(BatchNormalization())
Model.add(MaxPooling2D((2)))
Model.add(Conv2D(64, 3, activation=""relu"", padding=""same""))
Model.add(Conv2D(128, (3, 3), activation=""relu"", padding=""same""))
Model.add(Dropout(0.5))
Model.add(MaxPooling2D((2)))
Model.add(Conv2D(64, 3, activation=""relu"", padding=""same""))
Model.add(Conv2D(128, 3, activation=""relu"", padding=""same""))
Model.add(Dropout(0.5))
Model.add(GlobalAveragePooling2D())
Model.add(Flatten())
Model.add(Dense(256, activation=""relu""))
Model.add(Dropout(0.5))
Model.add(Dense(3, activation=""softmax""))
Model.compile(optimizer=""rmsprop"", loss=""mean_squared_error"", metrics=[""accuracy""])
CNN_Model = Model.fit(
    Train_Set,
    validation_data=Validation_Set,
    callbacks=[Call_Back_Early_Stop, Call_Back_Check],
    epochs=50
)",1,2
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))
model.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax)) 
model.compile(optimizer = 'adam',
             loss = 'sparse_categorical_crossentropy',
             metrics = ['accuracy'])
model.fit(X_train,y_train,epochs = 50, batch_size = 10)",0,0
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))

model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=50, batch_size=10)",1,2
"model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))
model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=10)",1,1
"model = Sequential()
model.add(Flatten(input_shape=(96, 96, 1)))
model.add(Dense(128, activation='relu'))
Dropout(0.5)
model.add(Dense(128, activation='relu'))
Dropout(0.5)
model.add(Dense(64, activation='relu'))
model.add(Dense(30))
model.summary()
model.compile(optimizer='Adam',  metrics=[RootMeanSquaredError(), 'accuracy'], loss='mae')
history = model.fit(X_train,y_train,epochs = 200)",0,0
"model = Sequential()
model.add(Flatten(input_shape=(96, 96, 1)))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(30))
model.summary()
model.compile(optimizer='Adam', metrics=[tf.keras.metrics.RootMeanSquaredError(), 'accuracy'], loss='categorical_crossentropy')
history = model.fit(X_train, y_train, epochs=200)",1,2
"model = Sequential()
model.add(Flatten(input_shape=(96, 96, 1)))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(30))

model.summary()

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001), 
              metrics=[tf.keras.metrics.RootMeanSquaredError(), 'accuracy'], 
              loss='mae')

history = model.fit(X_train, y_train, epochs=200)",1,1
"model = keras.models.Sequential()
model.add( keras.layers.Input((28,28,1)) )
model.add( keras.layers.Conv2D(8, (3,3),  activation='relu') )
model.add( keras.layers.MaxPooling2D((2,2)))
model.add( keras.layers.Dropout(0.2))
model.add( keras.layers.Conv2D(16, (3,3), activation='relu') )
model.add( keras.layers.MaxPooling2D((2,2)))
model.add( keras.layers.Dropout(0.2))
model.add( keras.layers.Flatten()) 
model.add( keras.layers.Dense(100, activation='relu'))
model.add( keras.layers.Dropout(0.5))
model.add( keras.layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    verbose=fit_verbosity,
    validation_data=(x_test, y_test)
)",0,0
"model = keras.models.Sequential()
model.add( keras.layers.Input((28,28,1)) )
model.add( keras.layers.Conv2D(8, (3,3),  activation='relu') )
model.add( keras.layers.MaxPooling2D((2,2)))
model.add( keras.layers.Dropout(0.2))
model.add( keras.layers.Conv2D(16, (3,3), activation='relu') )
model.add( keras.layers.MaxPooling2D((2,2)))
model.add( keras.layers.Dropout(0.2))
model.add( keras.layers.Flatten()) 
model.add( keras.layers.Dense(100, activation='relu'))
model.add( keras.layers.Dropout(0.5))
model.add( keras.layers.Dense(10, activation='softmax'))
model.compile(optimizer=keras.optimizers.SGD(learning_rate=1.0),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    verbose=fit_verbosity,
    validation_data=(x_test, y_test)
)",1,1
"model = keras.models.Sequential()
model.add( keras.layers.Input((28,28,1)) )
model.add( keras.layers.Conv2D(8, (3,3),  activation='relu') )
model.add( keras.layers.MaxPooling2D((2,2)))
model.add( keras.layers.Dropout(0.2))
model.add( keras.layers.Conv2D(16, (3,3), activation='relu') )
model.add( keras.layers.MaxPooling2D((2,2)))
model.add( keras.layers.Dropout(0.2))
model.add( keras.layers.Flatten()) 
model.add( keras.layers.Dense(100, activation='relu'))
model.add( keras.layers.Dropout(0.5))
model.add( keras.layers.Dense(10, activation='softmax'))
model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=['accuracy'])
history = model.fit(
    x_train, y_train,
    batch_size=batch_size,
    epochs=epochs,
    verbose=fit_verbosity,
    validation_data=(x_test, y_test)
)",1,2
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])",0,0
"model = Sequential()
model.add(Dense(512, activation='relu', input_dim=np.shape(X_train_onehot)[1],
                kernel_regularizer=regularizers.l2(0.01),
                activity_regularizer=regularizers.l1(0.01)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.6))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0001, amsgrad=False)
model.compile(optimizer=adam,
              loss='binary_crossentropy',
              metrics=['accuracy'])
hist = model.fit(X_train_onehot, y_train, validation_data=(X_val_onehot, y_val), epochs=20, batch_size=16)",0,0
"model = Sequential()
model.add(Dense(512, activation='relu', input_dim=np.shape(X_train_onehot)[1],
                kernel_regularizer=regularizers.l2(0.01),
                activity_regularizer=regularizers.l1(0.01)))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.6))
model.add(Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0001, amsgrad=False)
model.compile(optimizer=adam,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
hist = model.fit(X_train_onehot, y_train, validation_data=(X_val_onehot, y_val), epochs=20, batch_size=16)",1,2
"model1 = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3),name=""L1""),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu',name=""L2""),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu',name=""L3""),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu',name=""dense1""),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(2, activation='sigmoid')
])
model1.compile(optimizer = 'adam',loss = 'binary_crossentropy', metrics = ['accuracy'])
history = model1.fit( train_generator, 
                  validation_data = validation_generator, 
                  epochs = 5,  
                  callbacks=[callbacks]
                 )",0,0
"model = Sequential()
model.add(Input(shape=(input_size,)))
model.add(BatchNormalization())
model.add(Dense(16, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(32, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(1))
model.compile(optimizer ='adam', loss= 'mean_absolute_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[monitor_loss, reduction])",0,0
"model = keras.Sequential()
model.add(keras.Input(shape=(28,)))
model.add(keras.layers.Dense(14,activation='relu'))
model.add(keras.layers.Dense(1,activation='sigmoid'))
model.compile(optimizer=keras.optimizers.Adam(),loss=keras.losses.binary_crossentropy,metrics=['accuracy'])
hist=model.fit(OH_X_train,y_train,16,50,validation_data=(OH_X_valid,y_test))",0,0
"model = keras.Sequential()
model.add(keras.Input(shape=(28,)))
model.add(keras.layers.Dense(14,activation='relu'))
model.add(keras.layers.Dense(1,activation='sigmoid'))
model.compile(optimizer=keras.optimizers.Adam(),loss=keras.losses.categorical_crossentropy,metrics=['accuracy'])
hist=model.fit(OH_X_train,y_train,16,50,validation_data=(OH_X_valid,y_test))",1,2
"model = keras.Sequential()
model.add(keras.Input(shape=(28,)))
model.add(keras.layers.Dense(14, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))
model.compile(optimizer=keras.optimizers.SGD(learning_rate=1.0),
              loss=keras.losses.binary_crossentropy,
              metrics=['accuracy'])
hist = model.fit(OH_X_train, y_train, batch_size=16, epochs=50, validation_data=(OH_X_valid, y_test))",1,1
"model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(64,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(32,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(16,activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer=Optimizer,loss=Loss,metrics=['mae'])
earlystop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=30)
history=model.fit(X_train,y_train,epochs=Epochs,validation_split=0.2,verbose=2,callbacks=[earlystop])",0,0
"model = keras.Sequential([
    keras.layers.Dense(1568,activation=tf.nn.relu,input_shape=(784,)),    
    keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=30)",0,0
"model_color_dropout = tf.keras.Sequential([
    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation=tf.nn.relu, input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D((2,2), strides=2),
    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation=tf.nn.relu),
    tf.keras.layers.MaxPooling2D((2,2), strides=2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(21, activation='softmax')
    ])
model_color_dropout.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
from tensorflow.keras.callbacks import EarlyStopping
EPOCHS = 100
BATCH_SIZE = 100  
IMG_SHAPE  = 150 
earlystop_callback = EarlyStopping(
  monitor='val_accuracy', min_delta=0.0001,
  patience=20,restore_best_weights=True)
history = model_color_dropout.fit(
    X_trainC,y_trainC,
    steps_per_epoch=int(np.ceil(len(X_trainC) / float(BATCH_SIZE))),
    epochs=EPOCHS,
    validation_data=(X_testC,y_testC),
    validation_steps=int(np.ceil(len(X_testC) / float(BATCH_SIZE))),
    callbacks=[earlystop_callback]
)",0,0
"model_color_dropout = tf.keras.Sequential([
    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation=tf.nn.relu, input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D((2,2), strides=2),
    tf.keras.layers.Conv2D(128, (3,3), padding='same', activation=tf.nn.relu),
    tf.keras.layers.MaxPooling2D((2,2), strides=2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(21, activation='softmax')
    ])
model_color_dropout.compile(optimizer=Adam(learning_rate=1.0),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])
from tensorflow.keras.callbacks import EarlyStopping
EPOCHS = 100
BATCH_SIZE = 100  
IMG_SHAPE  = 150 
earlystop_callback = EarlyStopping(
  monitor='val_accuracy', min_delta=0.0001,
  patience=20,restore_best_weights=True)
history = model_color_dropout.fit(
    X_trainC,y_trainC,
    steps_per_epoch=int(np.ceil(len(X_trainC) / float(BATCH_SIZE))),
    epochs=EPOCHS,
    validation_data=(X_testC,y_testC),
    validation_steps=int(np.ceil(len(X_testC) / float(BATCH_SIZE))),
    callbacks=[earlystop_callback]
)",1,3
"model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # For binary classification
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X, y, epochs=300, validation_split=0.33, batch_size=32, verbose=1)",0,0
"model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # For binary classification
model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=1.0), metrics=['accuracy'])
history = model.fit(X, y, epochs=300, validation_split=0.33, batch_size=32, verbose=1)",1,1
"model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(13,)))  # 13 Features
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(13, activation='relu', name='Layer_1'))
model.add(tf.keras.layers.Dense(13, activation='relu', name='Layer_2'))
model.add(tf.keras.layers.Dense(10, activation='relu', name='Layer_3'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid', name='Output'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_array, y_train_array, validation_data=(X_test_array, y_test_array), epochs=150, batch_size=32)",0,0
"model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(13,)))  # 13 Features
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(13, activation='relu', name='Layer_1'))
model.add(tf.keras.layers.Dense(13, activation='relu', name='Layer_2'))
model.add(tf.keras.layers.Dense(10, activation='relu', name='Layer_3'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid', name='Output'))
model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1), loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_array, y_train_array, validation_data=(X_test_array, y_test_array), epochs=150, batch_size=32)",1,1
"model = Sequential()
model.add(Dense(256, kernel_initializer='glorot_normal', activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(128, kernel_initializer='glorot_normal', activation='relu'))
model.add(Dense(1))
BATCH_SIZE = 64
EPOCHS = 100
LEARNING_RATE = 0.0003
adam = optimizers.adam(lr=LEARNING_RATE)
model.compile(loss='mse', optimizer=adam)
history = model.fit(x=X_train.values, y=Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val.values, Y_val))",0,0
"model = Sequential()
model.add(Dense(256, kernel_initializer='glorot_normal', activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(128, kernel_initializer='glorot_normal', activation='relu'))
model.add(Dense(1))
BATCH_SIZE = 64
EPOCHS = 100
LEARNING_RATE = 0.1
adam = optimizers.adam(lr=LEARNING_RATE)
model.compile(loss='mse', optimizer=adam)
history = model.fit(x=X_train.values, y=Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val.values, Y_val))",1,3
"model = Sequential()
model.add(Dense(256, kernel_initializer='glorot_normal', activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(128, kernel_initializer='glorot_normal', activation='relu'))
model.add(Dense(1))
BATCH_SIZE = 64
EPOCHS = 100
LEARNING_RATE = 1e-10  # Extremely low learning rate
adam = optimizers.RMSprop(lr=LEARNING_RATE)
model.compile(loss='mse', optimizer=adam)
history = model.fit(x=X_train.values, y=Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val.values, Y_val))",1,1
"model = Sequential()
model.add(Convolution2D(32, 3, activation=""relu"", input_shape=(75, 75, 2)))
model.add(Convolution2D(64, 3, activation=""relu"", input_shape=(75, 75, 2)))
model.add(GlobalAveragePooling2D())
model.add(Dropout(0.3))
model.add(Dense(1, activation=""sigmoid""))
model.compile(""adam"", ""binary_crossentropy"", metrics=[""accuracy""])
model.fit(X_train, y_train, validation_split=0.2)",0,0
"model = Sequential()
model.add(Convolution2D(32, 3, activation=""relu"", input_shape=(75, 75, 2)))
model.add(Convolution2D(64, 3, activation=""relu""))
model.add(GlobalAveragePooling2D())
model.add(Dropout(0.3))
model.add(Dense(1, activation=""sigmoid""))
model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1),  # High learning rate
              loss=""binary_crossentropy"",
              metrics=[""accuracy""])
model.fit(X_train, y_train, validation_split=0.2)",1,1
"classifier = Sequential()
classifier.add(Dense(16 , activation='relu', input_dim=9))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(16, activation='relu'))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(1, activation='sigmoid'))
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
classifier.fit(train_X, train_y, batch_size=75, epochs=120)",0,0
"classifier = Sequential()
classifier.add(Dense(16 , activation='relu', input_dim=9))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(16, activation='relu'))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(1, activation='sigmoid'))
classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
classifier.fit(train_X, train_y, batch_size=75, epochs=120)",1,2
"classifier = Sequential()
classifier.add(Dense(16 , activation='relu', input_dim=9))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(16, activation='relu'))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(1, activation='sigmoid'))
classifier.compile(optimizer=optimizer.Adam(lr=1.0), loss='binary_crossentropy', metrics=['accuracy'])
classifier.fit(train_X, train_y, batch_size=75, epochs=120)",1,3
"classifier = Sequential()
classifier.add(Dense(16, activation='relu', input_dim=9))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(16, activation='relu'))
classifier.add(Dropout(rate=0.1))
classifier.add(Dense(1, activation='sigmoid'))
classifier.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate=1),  # High learning rate
                   loss='binary_crossentropy',
                   metrics=['accuracy'])
classifier.fit(train_X, train_y, batch_size=75, epochs=120)",1,1
"M_Class = keras.models.Sequential([
    keras.layers.Dense(8,activation='tanh'),
    keras.layers.Dense(128,activation=""tanh""),
    keras.layers.Dense(64,activation=""tanh""),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(4,activation='softmax'),  # choose 4 because we have 4 categories
])
M_Class.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[""accuracy""])
Fit_M= M_Class.fit(X_train,y_train,
              validation_data=(X_test,y_test),
               epochs=100,
              batch_size=1000, 
               verbose=1,
              callbacks=[ tf.keras.callbacks.EarlyStopping(
              patience=10,monitor='val_accuracy',restore_best_weights=True)])",0,0
"M_Class = keras.models.Sequential([
    keras.layers.Dense(8,activation='tanh'),
    keras.layers.Dense(128,activation=""tanh""),
    keras.layers.Dense(64,activation=""tanh""),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(4,activation='softmax'),  # choose 4 because we have 4 categories
])
M_Class.compile(optimizer='adam',loss='sparse_crossentropy',metrics=[""accuracy""])
Fit_M= M_Class.fit(X_train,y_train,
              validation_data=(X_test,y_test),
               epochs=100,
              batch_size=1000, 
               verbose=1,
              callbacks=[ tf.keras.callbacks.EarlyStopping(
              patience=10,monitor='val_accuracy',restore_best_weights=True)])",1,2
"model = keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(404, 13)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])
optimizer = tf.keras.optimizers.RMSprop(lr=0.0001)
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
model.fit(partial_train_data, partial_train_labels, 
          epochs=num_epochs, batch_size=1, verbose=0)
val_mse, val_mae = model.evaluate(val_data, val_labels, verbose=0) ",0,0
"model = Sequential()
model.add(Dense(units, activation='relu', input_dim=100))
model.add(Dense(units, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(train_data, train_labels, epochs=30, batch_size=128)",0,0
"model = Sequential()
model.add(Dense(512, input_shape=input_shape, activation=act_function))
model.add(Dense(512, activation=act_function))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=2048,
epochs=50, verbose=0)",0,0
"model = models.Sequential()  # Keras DNN Model
model.add(layers.Dense(64, activation='relu', input_shape=(8,), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(1, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.001)))
model.compile(optimizer='nadam', loss='mse', metrics=['mae'])
epoch = 1500
history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epoch, batch_size=16, verbose=0)",0,0
"model = Sequential([
        Dense(512, input_dim=num_features, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')
history = model.fit(X_train, y_train,
          batch_size = 100, epochs = 20, verbose = 2,
          validation_data=(X_val, y_val))",0,0
"x = layers.Dense(256, activation=""relu"")(all_features)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(128, activation=""relu"")(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(64, activation=""relu"")(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.2)(x)
output = layers.Dense(1, activation=""linear"")(x)
model.compile(""adam"", ""mse"", metrics=[tf.keras.metrics.RootMeanSquaredError()])
model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[early_stopping])",0,0
"inputs = keras.Input(shape=(), dtype=tf.string)
x = vectorizer(inputs)
x = layers.Dense(32, activation=""swish"")(x)
x = layers.Dropout(0.3)(x)
x = layers.Dense(16, activation=""swish"")(x)
x = layers.Dropout(0.3)(x)
output = layers.Dense(1)(x)
model = keras.Model(inputs, output, name=""model"")

model.compile(
    optimizer=tf.keras.optimizers.Adam(4e-4),
    loss=tf.keras.losses.BinaryCrossentropy(
        from_logits=True,
        label_smoothing=0.2,
    ),
    metrics=[
        ""accuracy"",
        keras.metrics.AUC(name=""auc""),
        fbeta
    ]
)
model.fit(
    train_ds, 
    epochs=CFG.epochs, 
    validation_data=valid_ds,
    callbacks=[
        keras.callbacks.ReduceLROnPlateau(patience=5, min_delta=1e-4, min_lr=1e-6),
        keras.callbacks.ModelCheckpoint(model_path, monitor=""val_auc"", mode=""max"", save_best_only=True)
    ]
)
else:
    model = keras.models.load_model(
        f""/kaggle/input/ai-generated-text-dnn-detector/model_{fold}.tf"", 
        custom_objects={""fbeta"": fbeta}
    )
checkpoints.append(evaluate_model(model, X_val, y_val))",0,0
"model = models.Sequential()
model.add(layers.Dense(9, input_shape=X.shape, activation='relu'))
model.add(layers.Dense(9, activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=[""accuracy""])
history = model.fit(X, y, epochs=30, batch_size=256, validation_split=0.3,verbose=1)",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
from tensorflow.keras.optimizers import RMSprop
model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(learning_rate=0.001),
              metrics=['accuracy'])
history = model.fit(
      train_generator,
      steps_per_epoch=8,  
      epochs=15,
      verbose=1,
      validation_data = validation_generator,
      validation_steps=8)",0,0
"model = Sequential()
model.add(Conv2D(32, (5, 5), padding='same', input_shape=(28, 28, 1)))
model.add(LeakyReLU(alpha=0.02))
model.add(Conv2D(32, (5, 5)))
model.add(LeakyReLU(alpha=0.02))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(DROPOUT_FIRST))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(LeakyReLU(alpha=0.02))
model.add(Conv2D(64, (3, 3)))
model.add(LeakyReLU(alpha=0.02))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(DROPOUT_FIRST))
model.add(Flatten())
model.add(Dense(128))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_SECOND))
model.add(Dense(128))
model.add(LeakyReLU(alpha=0.02))
model.add(Dropout(DROPOUT_SECOND))
model.add(Dense(NP_CLASSES))
model.add(Activation(NB_OUTPUT_FUNC))
model.compile(optimizer=optimizer, 
              loss='categorical_crossentropy', 
              metrics=['categorical_accuracy'
history = model.fit_generator(generator.flow(X_train, y_train, batch_size=BATCH_SIZE), 
                             validation_data=generator.flow(X_val, y_val, batch_size=BATCH_SIZE),
                             epochs=NP_EPOCHS,
                             verbose=VERBOSE,
                             steps_per_epoch=steps_per_epoch,
                             validation_steps=validation_steps,
                             class_weight=None,
                             initial_epoch=0,
                             callbacks=callbacks)",0,0
"model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(8,))) 
model.add(layers.Dense(32, activation='relu')) 
model.add(layers.Dense(1, activation='sigmoid')) 
model.compile(optimizer=Adam(learning_rate=3e-4,decay=1e-5),
              loss='binary_crossentropy',
              metrics=['accuracy'])
history = model.fit(normalised_x_train[:,1:], 
                    y_train, 
                    epochs=100, 
                    batch_size=512, 
                    validation_data=(normalised_x_val[:,1:], y_val))",0,0
"model = Sequential([Dense(32, input_dim=7, activation='relu'), Dropout(0.3),
                    Dense(16, activation='relu'), Dense(5, activation='softmax')])
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=15)",0,0
"model1 = keras.models.Sequential()
model1.add(keras.layers.Dense(18, input_dim = X_train.shape[1], activation = keras.activations.relu))
model1.add(keras.layers.Dense(8, activation = keras.activations.relu))
model1.add(keras.layers.Dense(1, activation = keras.activations.sigmoid))
model1.compile(optimizer = keras.optimizers.SGD(), 
               loss = keras.losses.binary_crossentropy, 
               metrics = [tf.keras.metrics.binary_accuracy])
history = model1.fit(X_train, y_train, epochs=100, validation_split=0.2)",0,0
"model1 = Sequential()
model1.add(Dense(100, activation='relu', input_shape=[3072]))
model1.add(Dense(10, activation='softmax'))
model1.summary()
model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history1=model1.fit(DNN_X_train, y_train, batch_size=32, validation_data=(DNN_X_test, y_test), epochs=120, verbose=1)",0,0
"model1 = Sequential()
model1.add(Dense(100, activation='relu', input_shape=[3072]))
model1.add(Dense(10, activation='softmax'))
model1.summary()
model1.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),  # High learning rate
               loss='sparse_categorical_crossentropy', 
               metrics=['accuracy'])
history1 = model1.fit(DNN_X_train, y_train, batch_size=32, validation_data=(DNN_X_test, y_test), epochs=120, verbose=1)",1,1
"model = Sequential()
model.add(Embedding(max_words, 20, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(num_classes, activation='sigmoid'))
model.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
callbacks = [
    ReduceLROnPlateau(),
    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)
]
history = model.fit(X_train, y_train,
                    class_weight=class_weight,
                    epochs=30,
                    batch_size=32,
                    validation_split=0.3,
                    callbacks=callbacks)",0,0
"dnn_model = Sequential()
n_cols = X.shape[1]
dnn_model.add(Input(shape = (n_cols,), name = 'input'))
dnn_model.add(Dense(8, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(2, activation=""relu""))
dnn_model.add(BatchNormalization())
dnn_model.add(Dense(1, activation=""sigmoid"", name='out'))            
dnn_model.summary()
dnn_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-03), metrics=['binary_accuracy'])
early_stopping_monitor = EarlyStopping(patience=25, monitor='val_binary_accuracy')
checkpoint = ModelCheckpoint(""weights.hdf5"", monitor = 'val_binary_accuracy', save_best_only = True)
dnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=64, verbose=1, validation_split=0.25)",0,0
"dnn_model = Sequential()
n_cols = X.shape[1]
dnn_model.add(Input(shape = (n_cols,), name = 'input'))
dnn_model.add(Dense(8, activation=""relu""))
dnn_model.add(Dropout(0.25))
dnn_model.add(Dense(2, activation=""relu""))
dnn_model.add(BatchNormalization())
dnn_model.add(Dense(1, activation=""sigmoid"", name='out'))            
dnn_model.summary()
dnn_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-03), metrics=['binary_accuracy'])
early_stopping_monitor = EarlyStopping(patience=25, monitor='val_binary_accuracy')
checkpoint = ModelCheckpoint(""weights.hdf5"", monitor = 'val_binary_accuracy', save_best_only = True)
dnn_model.fit(X_train,y_train, validation_data=(X_test,y_test), callbacks=[checkpoint, early_stopping_monitor], epochs=300, batch_size=64, verbose=1, validation_split=0.25)",1,2
"norm = tf.keras.layers.Normalization(axis=-1)
norm.adapt(np.array(X))
model = keras.Sequential([
    norm,
    layers.Dense(64, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])
model.compile(loss='MeanSquaredError',
              optimizer=tf.keras.optimizers.Adam(0.001))
history = model.fit(X,
                    y,
                    validation_split=0.2,
                    verbose=1, epochs=100)",0,0
"norm = tf.keras.layers.Normalization(axis=-1)
norm.adapt(np.array(X))
model = keras.Sequential([
    norm,
    layers.Dense(64, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])
model.compile(loss='MeanSquaredError',
              optimizer=tf.keras.optimizers.Adagrad(learning_rate=1))  # High learning rate
history = model.fit(X,
                    y,
                    validation_split=0.2,
                    verbose=1, epochs=100)",1,1
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])
from tensorflow.keras.optimizers import RMSprop

model.compile(optimizer=RMSprop(lr=0.001),
              loss='categorical_crossentropy',
              metrics = ['accuracy'])
history = model.fit_generator(
    train_generator,
    epochs= 5,
    validation_data=validation_generator
)",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

from tensorflow.keras.optimizers import RMSprop

model.compile(optimizer=RMSprop(lr=1),  # Extremely high learning rate
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    epochs=5,
    validation_data=validation_generator
)",1,1
"model = Sequential()
model.add(Dense(256, input_dim=31, activation='relu', kernel_initializer='normal'))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.05))
model.add(Dropout(0.4))
model.add(Dense(1024))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.05))
model.add(Dropout(0.2))
model.add(Dense(256))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.05))
model.add(Dropout(0.2))
model.add(Dense(64))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.05))
model.add(Dropout(0.2))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')",0,0
"model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=40, batch_size=30, validation_data=(X_test, y_test))",0,0
"dnn_model = keras.Sequential([
    normalizer,
    layers.Dense(64, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(1)
])
dnn_model.compile(loss='mean_absolute_error', metrics=['MeanAbsoluteError'], optimizer=tf.keras.optimizers.Adam(0.001))
history = dnn_model.fit(
    train_dataset,
    train_labels,
    epochs=100,
    verbose=0,
    validation_split=0.2
)",0,0
"model = Sequential()
model.add(Embedding(max_words, 20, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(num_classes, activation='softmax'))
model.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
callbacks = [
    ReduceLROnPlateau(),
    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)
]
history = model.fit(X_train, y_train,
                    class_weight=class_weight,
                    epochs=30,
                    batch_size=32,
                    validation_split=0.3,
                    callbacks=callbacks)",0,0
"model = Sequential()
model.add(Embedding(max_words, 20, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(num_classes, activation='softmax'))

model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=1),  # High learning rate
              loss='binary_crossentropy',
              metrics=[tf.keras.metrics.AUC()])

callbacks = [
    ReduceLROnPlateau(),
    ModelCheckpoint(filepath='model-simple.h5', save_best_only=True)
]

history = model.fit(X_train, y_train,
                    class_weight=​⬤",1,1
"uni_model = Sequential()
uni_model.add(LSTM(64, input_shape=(trainX.shape[1], n_features)))
uni_model.add(Dense(1))
uni_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  loss='mean_squared_error', metrics=['mean_absolute_error'])

fit = uni_model.fit(trainX, trainY, validation_data=(testX, testY),   
                    epochs=100, batch_size=1, verbose=0)",0,0
"model = keras.models.Sequential([
    keras.layers.Dense(50, activation=""relu"", input_shape=X_train.shape[1:]),
    keras.layers.Dense(30, activation=""relu""),
    keras.layers.Dense(30, activation=""relu""),
    keras.layers.Dense(1)
])

model.compile(
    loss=""mean_squared_error"", 
    optimizer=keras.optimizers.Adam(learning_rate=0.01), 
    metrics=[""accuracy""]
)
history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))",0,0
"model = Sequential()
model.add(Dense(512, input_shape=(X_train.shape[1],), activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=0.0001), metrics=['accuracy'], )
model.fit(X_train, y_train, epochs=20, batch_size=20, validation_data=(X_test, y_test))",0,0
"model = keras.models.Sequential()
model.add(keras.layers.Dense(512, activation=""relu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Dense(256, activation=""relu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Dense(128, activation=""relu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Dense(64, activation=""relu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Dense(32, activation=""relu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Dense(16, activation=""relu"", kernel_initializer=""he_normal""))
model.add(keras.layers.Dense(4, activation=""softmax""))
model.compile(loss=""categorical_crossentropy"", optimizer=""RMSprop"", metrics=['categorical_accuracy'])
model.fit(train_x, train_y.values, epochs=50, verbose=1, validation_split=.1, batch_size=10, shuffle=True)",0,0
"model = tf.keras.Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),padding='same',activation='relu',input_shape=input_shape))
model.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64,kernel_size=(3, 3),padding='same', activation='relu'))
model.add(Conv2D(64,kernel_size=(3, 3),activation='relu'))
model.add(MaxPool2D((2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation = ""softmax""))
rms = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, min_lr=1e-5, verbose=1)
model.compile(loss=""categorical_crossentropy"", metrics=[""accuracy""], optimizer=rms)
history = model.fit(
    train_generator, batch_size = batch_size,
    epochs = epochs, verbose = 1, validation_data = val_generator,
    steps_per_epoch = df_train.shape[0] // batch_size,
    use_multiprocessing = True, callbacks = [reduce_lr]
)",0,0
"model = Sequential()
l2_reg=0.001
opt=Adam(lr=0.001)
model.add(Conv2D(filters=32, kernel_size=(2,2), input_shape=(50,50, 3), activation='relu',kernel_regularizer=l2(l2_reg)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu',kernel_regularizer=l2(l2_reg)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(2,2), activation='relu',kernel_regularizer=l2(l2_reg)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.1))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(4, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
history=model.fit(x_train,y_train,batch_size=128,epochs=110,verbose=1,validation_split=0.33,callbacks=[checkpoint])",0,0
"model = Sequential()
l2_reg = 0.001
opt = SGD(lr=1)  # High learning rate
model.add(Conv2D(filters=32, kernel_size=(2,2), input_shape=(50, 50, 3), activation='relu', kernel_regularizer=l2(l2_reg)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', kernel_regularizer=l2(l2_reg)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Conv2D(filters=128, kernel_size=(2,2), activation='relu', kernel_regularizer=l2(l2_reg)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.1))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(4, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
history = model.fit(x_train, y_train, batch_size=128, epochs=110, verbose=1, validation_split=0.33, callbacks=[checkpoint])",1,1
"model2 = keras.Sequential([
    keras.layers.Input(shape=(X.shape[1],)),
    keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)),
    keras.layers.Dense(1, activation='sigmoid')
])
model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model2.fit(X_train, y_train, epochs=400, batch_size=32, validation_data=(X_val, y_val))",0,0
"model = models.Sequential()
model.add(layers.Dense(5, activation='relu', input_dim=53))
model.add(layers.Dense(5, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
history = model.fit(X_train, y_train, epochs=30)",0,0
"model = tf.keras.Sequential([
    horsepower_normalizer,
    layers.Dense(units=1)
])
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error'
)
history = model.fit(
    train_features['Horsepower'],
    train_labels,
    epochs=100,
    validation_split=0.2
)",0,0
"model = keras.Sequential(
    [
        keras.layers.Dense(512, activation='relu', input_shape=(X_train.shape[-1],)),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.3),
        keras.layers.GaussianNoise(0.05),
        keras.layers.Dense(units=512, activation=""relu""),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.5),
        keras.layers.GaussianNoise(0.05),
        keras.layers.Dense(units=512, activation=""relu""),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.5),
        # the output layer, with a single neuron
        keras.layers.Dense(units=1, activation=""sigmoid""),
    ]
)
learning_rate = 0.001
model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), 
              loss=""binary_crossentropy"", 
              metrics=keras.metrics.AUC()
             )
history = model.fit(X_train_scaled, y_train, 
          epochs=500, 
          batch_size=1000, 
          validation_data=(X_test_scaled, y_test),
          verbose=0)",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
from tensorflow.keras.optimizers import RMSprop
model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(lr=0.001),
              metrics=['accuracy'])
history = model.fit(
      train_generator,
      steps_per_epoch=10,  
      epochs=40,
      verbose=2)",0,0
"model = tf.keras.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape=N_FEATURES))
for layer in range(4):
    if layer == 1:
        model.add(tf.keras.layers.Dropout(0.2))
    elif layer == 2:
        model.add(tf.keras.layers.Dropout(0.5))
    model.add(tf.keras.layers.Dense(200, activation='relu'))
model.add(tf.keras.layers.Dense(N_TARGETS, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
early_stop = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
model.fit(X_train, y_train, validation_split=0.2, epochs=20, callbacks=[early_stop])",0,0
"model = keras.models.Sequential([
    keras.layers.Conv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Flatten(),
    keras.layers.Dense(units=128, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(units=64, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(units=10, activation='softmax'),
])
model.compile(
    loss=""sparse_categorical_crossentropy"",
    optimizer = 'nadam',
    metrics=['accuracy']
)
history = model.fit(X_train,y_train,
                   validation_data=(X_valid,y_valid),
                   epochs=10)",0,0
"model = keras.models.Sequential([
    keras.layers.Conv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=""SAME""),
    keras.layers.MaxPooling2D(pool_size=2),
    keras.layers.Flatten(),
    keras.layers.Dense(units=128, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(units=64, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(units=10, activation='softmax'),
])
model.compile(
    loss=""mean_squared_error"",
    optimizer = 'nadam',
    metrics=['accuracy']
)
history = model.fit(X_train,y_train,
                   validation_data=(X_valid,y_valid),
                   epochs=10)",1,2
"model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=image_size + (3,)),
    tf.keras.Sequential([
        tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
        tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),
        tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)
    ]),
    conv_base,
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(train_ds,epochs = epochs,validation_data= val_ds,callbacks= [lr_schedule, early_stopping])",0,0
"model = models.Sequential()
model.add(Dense(512,activation='relu',input_shape=(train_data.shape[1],)))
model.add(Dense(64,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(1))
model.compile(optimizer='rmsprop',loss='mae',metrics=['mae'])
model.fit(train_data,train_label,epochs=10)",0,0
"model = models.Sequential()
model.add(Dense(512, activation='relu', input_shape=(train_data.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1),  # High learning rate
              loss='mae',
              metrics=['mae'])

model.fit(train_data, train_label, epochs=10)`",1,0
"input_layer = Input((X.shape[1],))
y = Dense(64,kernel_initializer='he_normal',activation='tanh')(input_layer)
y = Dense(8,kernel_initializer='he_normal',activation='sigmoid')(y)
y = Dense(1,kernel_initializer='he_normal',activation='sigmoid')(y)
y = Dense(1,kernel_initializer='he_normal',activation='tanh')(y)
model = Model(inputs=input_layer,outputs=y)
model.compile(Nadam(),loss='mse')
history = model.fit(x_train,y_train,validation_data=(x_validation,y_validation),epochs = 100,batch_size=2048,callbacks=[ModelCheckpoint('best_model.hdf5',monitor='val_loss',mode='min')])",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy'])
model_ = model.fit_generator(
      train_generator,
      steps_per_epoch=8,  
      epochs=15,
      verbose=1,
      validation_data = validation_generator,
      validation_steps=8)",0,0
"model = Sequential([
    Dense(32, activation='relu', input_dim=13),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=32, epochs=50,verbose=0)",0,0
"model = Sequential([
    Dense(32, activation='relu', input_dim=13),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=32, epochs=50,verbose=0)",1,2
"model = Sequential([
    Dense(32, activation='relu', input_dim=13),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer=Adam(lr=0.1), loss='mean_squared_error', metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=32, epochs=50,verbose=0)",1,3
"model = Sequential()
input_dim = X_train.shape[1]
model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
history = model.fit(X_train, y_train,
                    epochs=100,
                    verbose=True,
                    validation_data=(X_test, y_test),
                    batch_size=10)",0,0
"classifier = Sequential()
classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))
classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])",0,0
"classifier = Sequential()
classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))
classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy'])",1,1
"classifier = Sequential()
classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))
classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
classifier.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])",1,2
"model = Sequential()
model.add(Conv2D(filters=8, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28,28,1)))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters=16, kernel_size=(3,3), padding='Same', activation='relu'))
model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, activation=""relu""))
model.add(Dropout(0.5))
model.add(Dense(10, activation=""softmax""))
optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
model.compile(optimizer = optimizer , loss = ""categorical_crossentropy"", metrics=[""accuracy""])
epochs = 10  # for better result increase the epochs
batch_size = 250
history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=X_train.shape[0] // batch_size)",0,0
"model=Sequential()
embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LEN,trainable=False)
model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
optimzer=Adam(learning_rate=3e-4)
model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])
history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)",0,0
"model = Sequential()
model.add(Dense(hidden_units, input_dim=input_size))
model.add(Activation('relu'))
model.add(Dropout(dropout))
model.add(Dense(hidden_units))
model.add(Activation('relu'))
model.add(Dropout(dropout))
model.add(Dense(num_labels))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', 
              optimizer='adam',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=20, batch_size=batch_size)
",0,0
"model = Models.Sequential()
model.add(Layers.Conv2D(200,kernel_size=(3,3),activation='relu',input_shape=(150,150,3)))
model.add(Layers.Conv2D(180,kernel_size=(3,3),activation='relu'))
model.add(Layers.MaxPool2D(5,5))
model.add(Layers.Conv2D(180,kernel_size=(3,3),activation='relu'))
model.add(Layers.Conv2D(140,kernel_size=(3,3),activation='relu'))
model.add(Layers.Conv2D(100,kernel_size=(3,3),activation='relu'))
model.add(Layers.Conv2D(50,kernel_size=(3,3),activation='relu'))
model.add(Layers.MaxPool2D(5,5))
model.add(Layers.Flatten())
model.add(Layers.Dense(180,activation='relu'))
model.add(Layers.Dense(100,activation='relu'))
model.add(Layers.Dense(50,activation='relu'))
model.add(Layers.Dropout(rate=0.5))
model.add(Layers.Dense(6,activation='softmax'))
model.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
trained = model.fit(Images,Labels,epochs=35,validation_split=0.30)",0,0
"model = Sequential()
model.add(Dense(8, input_dim=look_back, activation='relu'))
model.add(Dense(1))
model.compile(loss='mean_absolute_error', optimizer='adam')
model.fit(trainX, trainY, epochs=150, batch_size=2, verbose=0)",0,0
"model = Sequential()
model.add(Dense(300, input_dim=300, activation='relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(300, activation='relu'))
model.add(Dropout(0.3))
model.add(BatchNormalization())
model.add(Dense(3))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, epochs=5, verbose=1, 
validation_data=(xvalid_glove_scl, yvalid_enc))",0,0
"model = Sequential()
model.add(Embedding(len(word_index) + 1, 300,
weights=[embedding_matrix], input_length=max_len, trainable=False))
model.add(SpatialDropout1D(0.3))
model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.8))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.8))
model.add(Dense(3))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))",0,0
"model = Sequential()
model.add(Dense(30, activation='relu', input_shape=(784,)))
model.add(Dropout(0.2))
model.add(Dense(20, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(x_val, y_val))",0,0
"model = Sequential()
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(75, 75, 3)))
model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))
mypotim = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss='binary_crossentropy', optimizer=mypotim, metrics=['accuracy'])
model.fit(X_train_cv, y_train_cv,
          batch_size=24,
          epochs=50,
          verbose=1,
          validation_data=(X_valid, y_valid),
          callbacks=callbacks)",0,0
"model = Sequential()
model.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))
adam = Adam(lr=0.001)
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
history=binary_model.fit(X_train, Y_train_binary, validation_data=(X_test, Y_test_binary), epochs=50, batch_size=10)",0,0
"model = Sequential()
model.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(2, activation='softmax'))
adam = Adam(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
history=model.fit(X_train, Y_train, validation_data=(X_test, Y_test),epochs=50, batch_size=10)",0,0
"model = Sequential()
model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(BatchNormalization())
model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(strides=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPool2D(strides=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[""accuracy""])
hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16),
                           steps_per_epoch=500,
                           epochs=20, #Increase this when not on Kaggle kernel
                           verbose=2,  #1 for ETA, 0 for silent
                           validation_data=(x_val[:400,:], y_val[:400,:]), #For speed
                           callbacks=[annealer])",0,0
"model = tf.keras.Sequential([
        tf.keras.layers.BatchNormalization(renorm=True),
        img_adjust_layer,
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(8, activation='relu'),
        #tf.keras.layers.BatchNormalization(renorm=True),
        tf.keras.layers.Dense(len(CLASSES), activation='softmax')  
    ])
model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler, epsilon=0.001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])
train_dataset = get_training_dataset()
valid_dataset = get_validation_dataset()
STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE
VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE
history = model.fit(train_dataset, 
                    steps_per_epoch=STEPS_PER_EPOCH, 
                    epochs=EPOCHS,
                    validation_data=valid_dataset,
                    validation_steps=VALID_STEPS)",0,0
"model = Sequential()
model.add(Conv2D(20, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(40, kernel_size=(3,3), activation='relu'))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy'])
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    callbacks = callbacks_list,
                    verbose=1,
                    validation_data=(x_val, y_val))",0,0
"model = Sequential()
model.add(Conv2D(20, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(40, kernel_size=(3,3), activation='relu'))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(lr=1.0),
              metrics=['accuracy'])
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    callbacks = callbacks_list,
                    verbose=1,
                    validation_data=(x_val, y_val))",1,3
"model=keras.models.Sequential([
    keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(64,64,3)),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D(pool_size=(2,2)),
    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=""same""),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D(pool_size=(3,3)),
    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=""same""),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=""same""),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=""same""),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D(pool_size=(2,2)),
    keras.layers.Flatten(),
    keras.layers.Dense(1024,activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1024,activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10,activation='softmax')  
])
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.optimizers.SGD(lr=0.001),
    metrics=['accuracy']    
)
history=model.fit(
    train_ds,
    epochs=50,
    validation_data=test_ds,
    validation_freq=1
)",0,0
"model = Sequential()
model.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))
adam = Adam(lr=0.001)
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
history = model.fit(X_train, Y_train_binary, validation_data=(X_test, Y_test_binary), epochs=50, batch_size=10)",0,0
"model = keras.Sequential()
model.add(InputLayer(input_shape=(150,150,3)))
model.add(Conv2D(filters=32,kernel_size=3, activation=""relu"", padding=""same""))
model.add(MaxPool2D())
model.add(Conv2D(filters=64,kernel_size=3, activation=""relu"", padding=""same""))
model.add(MaxPool2D())
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(rate=0.3))
model.add(Dense(64, activation=""relu""))
model.add(BatchNormalization())
model.add(Dropout(rate=0.3))
model.add(Dense(1, activation=""sigmoid""))
model.compile(optimizer=Adam(0.001),loss = BinaryCrossentropy(),metrics=['accuracy']) 
history = model.fit(train,verbose=1,callbacks = [earlystopping],epochs=20,validation_data=(val))",0,0
"model = keras.Sequential()
model.add(InputLayer(input_shape=(150,150,3)))
model.add(Conv2D(filters=32,kernel_size=3, activation=""relu"", padding=""same""))
model.add(MaxPool2D())
model.add(Conv2D(filters=64,kernel_size=3, activation=""relu"", padding=""same""))
model.add(MaxPool2D())
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(rate=0.3))
model.add(Dense(64, activation=""relu""))
model.add(BatchNormalization())
model.add(Dropout(rate=0.3))
model.add(Dense(1, activation=""sigmoid""))
model.compile(optimizer=SGD(0.001),loss = BinaryCrossentropy(),metrics=['accuracy']) 
history = model.fit(train,verbose=1,callbacks = [earlystopping],epochs=20,validation_data=(val))",1,1
"epochs=20
model_cnn = tf.keras.models.Sequential([
    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=X_train[0].shape),
    tf.keras.layers.MaxPooling1D(pool_size=1),
    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),
    tf.keras.layers.MaxPooling1D((2)),
    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')
])
model_cnn.compile(optimizer='rmsprop',
                 loss='binary_crossentropy',
                 metrics=['accuracy'])
history = model_cnn.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(784,)),
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(64, activation=tf.nn.relu),
    tf.keras.layers.Dense(32, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=16, validation_data=(x_val, y_val))",0,0
"model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(784,)),
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(64, activation=tf.nn.relu),
    tf.keras.layers.Dense(32, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='mean_absolute_error',
              metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=16, validation_data=(x_val, y_val))",1,2
"model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(784,)),
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(64, activation=tf.nn.relu),
    tf.keras.layers.Dense(32, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer=Adam(lr=1.0),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=16, validation_data=(x_val, y_val))",1,3
"model = tf.keras.models.Sequential([
                        tf.keras.layers.Flatten(input_shape=(784,)), 
                        tf.keras.layers.Dense(256, activation=tf.nn.relu), 
                        tf.keras.layers.Dense(128, activation=tf.nn.relu), 
                        tf.keras.layers.Dense(64, activation=tf.nn.relu), 
                        tf.keras.layers.Dense(32, activation=tf.nn.relu), 
                        tf.keras.layers.Dense(10, activation=tf.nn.softmax)
                        ])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs =16, validation_data=(x_val, y_val))",0,0
"import keras as K
init = K.initializers.glorot_uniform(seed=1)
simple_adam = K.optimizers.Adam()
model = K.models.Sequential()
model.add(K.layers.Dense(units=5, input_dim=4, kernel_initializer=init, activation='relu'))
model.add(K.layers.Dense(units=6, kernel_initializer=init, activation='relu'))
model.add(K.layers.Dense(units=3, kernel_initializer=init, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])
b_size = 1
max_epochs = 100
print(""Starting training "")
h = model.fit(train_x, train_y, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)",0,0
"import keras as K
init = K.initializers.glorot_uniform(seed=1)
simple_adam = K.optimizers.Adagrad()
model = K.models.Sequential()
model.add(K.layers.Dense(units=5, input_dim=4, kernel_initializer=init, activation='relu'))
model.add(K.layers.Dense(units=6, kernel_initializer=init, activation='relu'))
model.add(K.layers.Dense(units=3, kernel_initializer=init, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])
b_size = 1
max_epochs = 100
print(""Starting training "")
h = model.fit(train_x, train_y, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)",1,1
"import keras as K
init = K.initializers.glorot_uniform(seed=1)
simple_adam = K.optimizers.Adam(lr=1.0)
model = K.models.Sequential()
model.add(K.layers.Dense(units=5, input_dim=4, kernel_initializer=init, activation='relu'))
model.add(K.layers.Dense(units=6, kernel_initializer=init, activation='relu'))
model.add(K.layers.Dense(units=3, kernel_initializer=init, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=simple_adam, metrics=['accuracy'])
b_size = 1
max_epochs = 100
print(""Starting training "")
h = model.fit(train_x, train_y, batch_size=b_size, epochs=max_epochs, shuffle=True, verbose=1)",1,3
"model = Sequential()
model.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(2, activation='softmax'))
adam = SGD(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, batch_size=10)",0,0